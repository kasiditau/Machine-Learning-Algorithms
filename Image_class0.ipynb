{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Mar  7 20:35:12 2018', '__version__': '1.0', '__globals__': [], 'train_images': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)}\n",
      "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Mar  7 20:35:12 2018', '__version__': '1.0', '__globals__': [], 'test_images': array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 216, 149,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "train_images = scipy.io.loadmat('train_images.mat')\n",
    "train_labels = scipy.io.loadmat('train_labels.mat')\n",
    "test_images = scipy.io.loadmat('test_images.mat')\n",
    "test_labels = scipy.io.loadmat('test_labels.mat')\n",
    "print(train_images)\n",
    "print(test_images )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  :  (10000, 784)\n",
      "X_train[0]  :  (784,)\n",
      "y_train  :  (1, 10000)\n",
      "X_test  :  (1000, 784)\n",
      "y_test  :  (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_images\n",
    "#train_labels\n",
    "#type(data)\n",
    "#train_images['train_images'].shape\n",
    "#train_labels['train_labels'].shape\n",
    "X_train = train_images['train_images']\n",
    "y_train = train_labels['train_labels']\n",
    "X_test = test_images['test_images']\n",
    "y_test = test_labels['test_labels']\n",
    "print(\"X_train  : \",X_train.shape)\n",
    "print(\"X_train[0]  : \",X_train[0].shape)\n",
    "print(\"y_train  : \",y_train.shape)\n",
    "#print(\"y_train[0]  : \",y_train[0].shape)\n",
    "print(\"X_test  : \",X_test.shape)\n",
    "print(\"y_test  : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATcAAAE3CAYAAADPIgYyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAADEwAAAxMBPWaDxwAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARKUlEQVR4nO3da2ze5X3G8et+Dn58wKckJiYkJZzCoWWETgRIUQ9AW4a2sZZ2ncTUISG1bNKQqk2qtJW264u96NRO09C0oa6T9oJtqrS9gEUdokDbFKFmDawpoRRCgJDUwU7iQ+znsZ/Dfy8SQjqMuf7hsR/nt+/nVUgu3b6fA5f/9t8/3ynLMi0mpVSRNCqpuWgAAFafoqSxLMvmS0uERq/VR16uqGelNgUA78q8qtqlxzdLemWpcmtW1KPu1LtC2wKAd+nEF6JNSSp0dCMAsEwoNwAhUW4AQnrL99xSSp+RdEDSwryq6hbfcwNw9lnsym1CUrekMe6UAjhbLVZu6yTVVnojANBOb/myNMuyf5OklNLGld8OALQHNxQAhES5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACG95dxS4Fek5GezbFm2UFy7xs4e+/gWKzfw4FNnup2l5Xi+Uqls5bL6wpnupjPyvGdcZ/De4soNQEiUG4CQKDcAIVFuAEKi3ACERLkBCIlyAxAS5QYgJMoNQEiUG4CQGL/CklKxaGezRsPOFrZeaWef+/w5/rpVL1ee3WavWaq27Gz5kf+2s8syVpVn/CvHa6vkXwctx+NKJa+qUlaSTr4NuXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIiXIDEBLlBiAkxq+wJHfsRco3fnXg40N29s4bfmhnfzR+kZV7pTJqr5n12FGVbrnBzm75u4NWrvHyq/4GcpwSlef1yqM4POwFm017zeb0tJXLsjcfE1duAEKi3ACERLkBCIlyAxDSW8otpfS5lNIGSaPzMn85FgCsMotduY1L6pU0VlGO20QAsIosVm6Tki5Y6Y0AQDu95YeYsix7XJJSShtXfjsA0B7cUAAQEuUGICTGr7CkVq22LOsuXHPczn5q0D9RqrtQt3LfL/gnWh18bJOdbf6a/7he+Wa/lWs9vd1ec+3P/JGmgad/aWcnPni+nR3/dW8EbP1T9pIafnSflSs2j0tHTvyZKzcAIVFuAEKi3ACERLkBCIlyAxAS5QYgJMoNQEiUG4CQKDcAITGh8P9VSl4ux4Ejx3/3ejv72SufsLP76iN2dmPXUSv36Q0/sdfU7/vZ+5//kJ2dfWnQyhX6/Ndg7Hr/euXg7f7zmtX9w2SGd3u1UviDw/aa0wvewT/zc5PSjpPr26sDwFmEcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQUsreZrwmpbTxRt12oDv1rvCW8CvcManlkmP86n0/8T9XfnLYP/Qlj6K8/c5mXfaak82+M93OksYb3gEx9cyfkvzWC/5hMsfN8S9JKjT89+FHP/K0lbtjzS57za9ffJWVq2Vz2qkdm7Ise40rNwAhUW4AQqLcAIREuQEIiXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQuL0q9Uux/hTp71w/Fw7e2TgHDs71hiys2uLx61cf6Fqr7m5PGFnx5veSJUkFcstK7eQFe01/+K9D9nZ2hVlO1tOTTu7vfuQlfv03s/aa/bpJTv7Bq7cAIREuQEIiXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIifErtM1IxRt9kqTuVLezXalhZw/Vh63cC9XL7DV/Me2Pld26/lk7WzfHqtwTvaR8Y1IbysfsbC3zR7XcV/YD6/2Rqmfs5Ju4cgMQEuUGICTKDUBIlBuAkE7dUEgp3SupS9KDWZYdkjQ6r6q61duxzQHAmTr9yu2opEckXXryv8cq6ln5HQFAG5xebr2SbpX0Yof2AgBtc+rL0izLHujkRgCgnbihACAkyg1ASIxfrXYp+dGif0pS1vBGmorD3jiTJH1oaI+dHW8O2NnJpn/Hfqg4Z+VmGt32mker/se/vPJLO7t7brOVG+nyx6Tcxy9JLy+ss7OXVsbs7NcP32zlNnUftdds3PxBL1ebknbukMSVG4CgKDcAIVFuAEKi3ACERLkBCIlyAxAS5QYgJMoNQEiUG4CQmFBY7TL/cJBU8l9Od0LhwN1X2Gve1PuQnX2ydr6dHSnN2Fn30JXzKlP2mv3ra3Y2zzTFmpJ3oM5M0//VY72FeTub53l9f9eEnf3Co++3cv3vO2KvOVD2rsOyxpsTPVy5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuNXq1wqd9nZVs0fE3Kt27NgZyeaZTs7VPAPMulKTTu7YI5fbV+z315zPMf40+7qhXa2v1i1ciMFf0xqU9kfadpT22Rnd8xeYmfv/s1Hrdy/PPBRe82u7z5p5VrZm+8rrtwAhES5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQipM+NXKb1zRlIq+eM8qZijpwt+tlUzTxNq+SNCeWR1f/xpOfzNP9xvZw80huzsWN3PDhX9Ua2mvPfWU9VBe83uQt3OjpSm7ex0yx/rcs20uu2se1KYlO85+OLaF6zcv0/dYq95JrhyAxAS5QYgJMoNQEiUG4CQKDcAIVFuAEKi3ACERLkBCIlyAxAS5QYgpLaNX6WSv1TWaHi5HKNHmT8dclap3r7Nzh74HX8E7M5rfmzlxhr99ppPz222s4PmyU+S1FcwR+Ak1TJvZO/QwrC9Zp7RozWl43b2XHNUq5n51yAH6/7jyiPPCNxrDe85mPlt/1SvoX+2o6dw5QYgJMoNQEiUG4CQKDcAIZ0qt5TSvSml+1JKlZN/NTov/5u+ALCanH7l9lNJU5LeuN00VlH7f5keAKyE08tti6RhSSMd2gsAtM2pH07LsuyBTm4EANqJGwoAQqLcAITUtvErd6RquZTOG7Wz9QvX29mjV/RaublR79QlSdp623N29q71/2Rnx5sDdracvNfrQH2tveY1vS/b2cemrrSzE6Vz7Kw71rW9zzuhSZImW957QJI2lI7Z2S+++Ckrt77XH1P61gU77Gw9a9nZ5+uVdw6dNNXyTtW698rH7TX/4wxuBXDlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIiXIDEBLlBiCktk0ozP/GtXb23D9/ycptHXjNXvPKnp12ttbyDhGR/MNB9lbPt9eca3XZ2RcW/MmLqYb/k/TF5P10+usL/gEx39h/i5393ra/t7NfOnSrnS30ZFbuSNOferjjHO8glxP899bn3/MDK3dR1+v2mg/PnmdnD+U4TGZ9ecrObi6PW7lP9v/CXpMJBQA4iXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIiXIDENKS41epVFJK3oTWdX+5y/6gN/c/a+XmMv9QijwjVXnGTlyDpTk7O1/3p95er/uHvuSxpTJm5T4x8Iy95g/uv87O3lj7Yzu77yb/kJzvVb3DScYb/vP6e/tvsrO7X91kZ6/fvN/KXdV/0F4zzwhef7FmZ90DhSRptuX9f/tUzR+BOxNcuQEIiXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIiXIDENKSc0CH796qcv+QtdBXB//W/qAPHr3eym3qPmqveUHXhJ29uucVO+vqL/ijLJcN+KMsD89utLNPTF5uZ88rT1q5H85dbK/5r1/9Kzt71xf+xM7esOMeOzu92ft83ejzTsmSpIGrj9jZL13zn3a2KzWt3GTTH6laU5m1s0NFf2QwD3dssr9QtdcsXnaJl6tPS/tO/JkrNwAhUW4AQqLcAIREuQEIiXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQlpy/KpnvKWuuZa10MPTW+0PelHPuJWbqPfba/7X8avs7MaeY3Z2sOiNiFxiniYlSc/UvJE2Sfru+Hvt7IaeaTt7uD5o5Y7U++w158xTjyTpH//6m3b2G4dvsbOfWLPbyl3d5Y9UTbb8a4C9C6N2dqbVbeVqmX+y21SOUa1+870tSfXMP7GtmHmdMVTwx7+mr1pr5ebnioxfAYiNcgMQEuUGICTKDUBIp8otpfSxlNKXU0obTv7V6MKc/w1qAFhNTr8F8nNJNUmXSjokaayrd6AjmwKAd+v0L0vvkrRN0oud2QoAtM+pK7csy77WyY0AQDtxQwFASEv+2HHfoQV1d89bC7WyZH/Qxya8g0zWd8/Ya27tP2Bnn5/zf4p8T3XDO4ck7S69x16zp1i3s4Nd/sEzfSXvtZKkdWXvub2w8rq9pnvgiSTtqvnP1x+OPGFnX20MW7mHZrfYa+6d894DkjRc8n/qfs+0t+5co8tec77pTxLUGv5Uz2DFfx9eu8Y7gOl5nWevOX61efDPVEE6eUYPV24AQqLcAIREuQEIiXIDEBLlBiAkyg1ASJQbgJAoNwAhUW4AQqLcAIS05KxG4amfqZC8Aye+88gH7A963+3fsXLfn/TGtCTp4TF/lGR6wT/IZKR31soNmONMkrSm7K0pSYM5xnm6U8POHmt4B7/MF/zDSZryR/DG5r0DaiTpR61L7Wy9VbRy82ZOyjcud3RhnZ3d0DNl5WYa3kEykvTyzBo7OzF1jp2t9fpjXTubF1u5W0eftdfsed17b9Vn3sxx5QYgJMoNQEiUG4CQKDcAIVFuAEKi3ACERLkBCIlyAxAS5QYgJMoNQEgpy7LF/yGljTfqtgPd5vhVHlN3Xm/lLvqj5+01tw3tt7O7p/2Tl141x1nqLf/zRLnQsrO95QU7251jTKir6J1UVdDi74/FtHKMX/UV/ceV51SvgZJ3SlN/0T/NqZD81yuPovnc/nhq87J8/P4cz2sj89/fNwzus3Lf3r/dXnPwNu+s+Fo2p53asSnLste4cgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuUGIKSlx68Kv+WPX7W8cZ7lMnvHdXb2uj/b5Wf7vVGSy7sO22uW5Y/zdOcY/ekr+ONPtbd53f+vPJ/9dlY32dlmjpUfO3aFna2bY0KH5wbsNcvmqFpercx7vaoN/wSyqap/Ulax4I/W1Z7wT/Vau9cbA6zs8P8/dDF+BSA8yg1ASJQbgJAoNwAhUW4AQqLcAIREuQEIiXIDEBLlBiAkyg1ASB05/SqidO1VdrY62mNnK0f8E4pmLvDXHdg3a+UK8w17zdb/PGdngeXA+BWA8Cg3ACFRbgBCotwAhHSq3FJKH04pfSWlVDn5V6PzqnZoWwDw7px+5bZXUibpjd+MN1aRf/cNAFaT08vtHp0ot5EO7QUA2qb0xh+yLPtaJzcCAO3EDQUAIZXeOQJHtmuPnfWP8Mhn4Mn2r+kfTwOsLly5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuUGICTKDUBIlBuAkCg3ACFRbgBCotwAhES5AQiJcgMQEuUGICTKDUBISx0QU5xX9cRJpgBwFphXVZKKkpSybPH2SilVJI1Kai7yz6OSxpZpf53E4zq78LjOHiv1mIqSxrIsm3/bK7csy+YlvbLYv6WUsizLDi7X7jqFx3V24XGdPTrxmN72yg0Azma5byiklD6TUtq+HJvppJTS51JKGzq9j3ZKKd2bUvrToI/rvpPfOgkjpfSxlNKXA75eH04pfWWlX68zuVs6oeU7NL2TxiX1dnoTbXZU0iOSLu30Rtrsp5KmJJU7vZE2+7mkJxTv9dqrE7cmV/T1OpNyWyep1u6NrAKTki7o9CbarFfSrZJe7PRG2myLpGFJI53eSJvdJWmb4r1e9+hEua3o68X33ACExA/xAgjpfwEV0Zcz0uDRFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displayign the image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(20,20), dpi=20)\n",
    "plt.imshow(X_train[0].reshape(28,28), cmap=\"viridis\")\n",
    "plt.figure(figsize=(20,20), dpi=20)\n",
    "plt.imshow(X_train[1].reshape(28,28), cmap=\"viridis\")#gray , plasma\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train tranposed shape: (10000, 1)\n",
      "y_test tranposed shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train tranposed shape:\" ,y_train.transpose().shape)\n",
    "y_train = y_train.transpose()\n",
    "print(\"y_test tranposed shape:\" ,y_test.transpose().shape)\n",
    "y_test = y_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long) \n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the unique label\n",
    "np.unique(y_train.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape:  torch.Size([10000])\n",
      "y_test shape:  torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "y_train =y_train.view(-1,)\n",
    "print(\"y_train shape: \",y_train.shape)\n",
    "y_test=y_test.view(-1,)\n",
    "print(\"y_test shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_features,num_hidden,num_out=3):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.input = nn.Linear(num_features, num_hidden)\n",
    "        self.hidden =nn.Linear(num_hidden,num_out)\n",
    "    def forward(self, x,):\n",
    "        X = F.relu(self.input(x))      \n",
    "        X = F.softmax(self.hidden(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self,l1,l2):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.input = nn.Linear(784,l1)\n",
    "        self.hidden1 = nn.Linear(l1,l2)\n",
    "        self.hidden2 = nn.Linear(l2,10)\n",
    "    def forward(self,x):\n",
    "        X = F.relu(self.input(x))\n",
    "        X = F.relu(self.hidden1(X))\n",
    "        X =self.hidden2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5310       0.1013        2.3074  1.9576\n",
      "      2        2.3066       0.1013        2.3062  0.4619\n",
      "      3        2.3054       0.1013        2.3049  0.4762\n",
      "      4        2.2976       0.1250        2.2773  0.4741\n",
      "      5        2.1221       0.1944        1.9477  0.4625\n",
      "      6        1.9164       0.1950        1.8980  0.5583\n",
      "      7        1.8770       0.1969        1.8671  0.5289\n",
      "      8        1.8529       0.2013        1.8476  0.4727\n",
      "      9        1.8333       0.2031        1.8343  0.4630\n",
      "     10        1.8177       0.2106        1.8161  0.4600\n",
      "     11        1.8053       0.2075        1.8068  0.4664\n",
      "     12        1.7927       0.2112        1.7944  0.5228\n",
      "     13        1.7803       0.2294        1.7845  0.4968\n",
      "     14        1.7716       0.2225        1.7792  0.5163\n",
      "     15        1.7624       0.2350        1.7693  0.4956\n",
      "     16        1.7517       0.2394        1.7591  0.4941\n",
      "     17        1.7237       0.2731        1.7430  0.5283\n",
      "     18        1.6959       0.2725        1.7312  0.4989\n",
      "     19        1.6774       0.3169        1.6625  0.5606\n",
      "     20        1.6572       0.3131        1.6582  0.4955\n",
      "     21        1.6518       0.3294        1.6540  0.4752\n",
      "     22        1.6358       0.3175        1.6486  0.4511\n",
      "     23        1.6312       0.3444        1.6378  0.4520\n",
      "     24        1.6250       0.3344        1.6316  0.4459\n",
      "     25        1.6130       0.3337        1.6314  0.4531\n",
      "     26        1.6073       0.3294        1.6234  0.4530\n",
      "     27        1.6032       0.3412        1.6129  0.4565\n",
      "     28        1.5932       0.3412        1.6130  0.4934\n",
      "     29        1.5903       0.3419        1.6286  0.5331\n",
      "     30        1.5865       0.3550        1.5979  0.5000\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5630       0.2856        1.8528  0.5024\n",
      "      2        1.6781       0.3200        1.6717  0.5762\n",
      "      3        1.5856       0.4213        1.6017  0.4989\n",
      "      4        1.5167       0.4475        1.5398  0.5267\n",
      "      5        1.4881       0.4537        1.5251  0.4626\n",
      "      6        1.4281       0.4575        1.5152  0.5070\n",
      "      7        1.4095       0.4556        1.4662  0.5363\n",
      "      8        1.3699       0.4619        1.4785  0.4783\n",
      "      9        1.3597       0.4600        1.4602  0.5347\n",
      "     10        1.1725       0.5525        1.3075  0.4444\n",
      "     11        1.1156       0.5550        1.2426  0.4594\n",
      "     12        1.0844       0.5700        1.2351  0.4699\n",
      "     13        1.0407       0.5881        1.1629  0.4604\n",
      "     14        0.9885       0.6081        1.1701  0.4513\n",
      "     15        0.9685       0.6050        1.1947  0.4651\n",
      "     16        0.9747       0.5894        1.1351  0.4484\n",
      "     17        0.9461       0.5869        1.1029  0.4432\n",
      "     18        0.9230       0.6450        1.0840  0.4521\n",
      "     19        0.9138       0.6200        1.0386  0.5705\n",
      "     20        0.8872       0.6312        1.0260  0.5331\n",
      "     21        0.8668       0.6238        1.0622  0.5208\n",
      "     22        0.8487       0.6012        1.0374  0.5024\n",
      "     23        0.8513       0.6144        1.0181  0.4942\n",
      "     24        0.8428       0.6300        0.9729  0.5625\n",
      "     25        0.8171       0.6344        1.0032  0.5269\n",
      "     26        0.8099       0.6375        1.0133  0.4744\n",
      "     27        0.7879       0.6587        1.0105  0.4640\n",
      "     28        0.7768       0.6625        0.9722  0.4760\n",
      "     29        0.7777       0.6631        0.9819  0.4775\n",
      "     30        0.7617       0.6562        1.0250  0.4747\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.8534       0.2137        2.0894  0.4805\n",
      "      2        1.9239       0.2950        1.8561  0.4630\n",
      "      3        1.8258       0.3137        1.8224  0.4620\n",
      "      4        1.7615       0.3312        1.7285  0.4546\n",
      "      5        1.6854       0.3762        1.6932  0.4576\n",
      "      6        1.5462       0.4462        1.5479  0.4593\n",
      "      7        1.4623       0.4725        1.4592  0.4589\n",
      "      8        1.4151       0.4763        1.4240  0.4612\n",
      "      9        1.3790       0.4781        1.3722  0.4602\n",
      "     10        1.3413       0.5069        1.3040  0.4629\n",
      "     11        1.1911       0.5725        1.2087  0.4775\n",
      "     12        1.1233       0.5719        1.1837  0.4567\n",
      "     13        1.0950       0.5913        1.1871  0.4658\n",
      "     14        1.0693       0.5819        1.1469  0.4585\n",
      "     15        1.0293       0.6156        1.0792  0.4539\n",
      "     16        1.0052       0.6144        1.1006  0.4592\n",
      "     17        0.9793       0.5881        1.1175  0.4529\n",
      "     18        0.9586       0.6038        1.0893  0.4639\n",
      "     19        0.9692       0.6050        1.0629  0.4551\n",
      "     20        0.9359       0.6181        1.0250  0.4568\n",
      "     21        0.9176       0.6175        1.0459  0.4612\n",
      "     22        0.9094       0.6088        1.0895  0.4552\n",
      "     23        0.8951       0.6281        1.0206  0.4550\n",
      "     24        0.8819       0.6169        1.0221  0.5380\n",
      "     25        0.8612       0.6150        1.0170  0.4866\n",
      "     26        0.8546       0.6256        1.0240  0.4684\n",
      "     27        0.8490       0.6331        1.0059  0.4548\n",
      "     28        0.8523       0.6175        1.0577  0.4557\n",
      "     29        0.8563       0.6319        0.9949  0.4504\n",
      "     30        0.8496       0.6269        0.9967  0.4530\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.7511       0.2975        1.8420  0.4596\n",
      "      2        1.6265       0.3969        1.4344  0.4759\n",
      "      3        1.3708       0.5000        1.3180  0.4815\n",
      "      4        1.1779       0.6244        1.1341  0.4763\n",
      "      5        1.0055       0.6644        1.0427  0.4649\n",
      "      6        0.9369       0.6644        0.9579  0.4653\n",
      "      7        0.8753       0.6856        0.9227  0.4613\n",
      "      8        0.8284       0.7025        0.8859  0.5314\n",
      "      9        0.8210       0.6444        0.9725  0.4984\n",
      "     10        0.8187       0.7275        0.8873  0.4944\n",
      "     11        0.7562       0.7006        0.8433  0.4814\n",
      "     12        0.7084       0.7562        0.7970  0.4822\n",
      "     13        0.6771       0.7588        0.7962  0.4579\n",
      "     14        0.6670       0.7475        0.7766  0.5332\n",
      "     15        0.6429       0.7644        0.7656  0.4931\n",
      "     16        0.5807       0.7619        0.6841  0.4904\n",
      "     17        0.5400       0.7837        0.6590  0.5277\n",
      "     18        0.5149       0.7750        0.6452  0.4725\n",
      "     19        0.4986       0.7650        0.6717  0.4848\n",
      "     20        0.4798       0.7688        0.6713  0.4706\n",
      "     21        0.4692       0.7887        0.6131  0.5058\n",
      "     22        0.4573       0.7794        0.6206  0.4515\n",
      "     23        0.4544       0.7856        0.6114  0.4569\n",
      "     24        0.4526       0.7719        0.6664  0.4542\n",
      "     25        0.4492       0.7669        0.6742  0.4785\n",
      "     26        0.4367       0.7819        0.6605  0.4891\n",
      "     27        0.4314       0.7925        0.6291  0.5193\n",
      "     28        0.4156       0.7750        0.6551  0.4890\n",
      "     29        0.4595       0.7925        0.6771  0.4756\n",
      "     30        0.4149       0.7900        0.6407  0.4724\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.8921       0.2125        2.0522  0.4632\n",
      "      2        1.9636       0.3337        1.8521  0.4523\n",
      "      3        1.8018       0.3269        1.8144  0.5206\n",
      "      4        1.7502       0.3500        1.7382  0.4455\n",
      "      5        1.7191       0.3638        1.7264  0.4581\n",
      "      6        1.6775       0.3669        1.6729  0.4497\n",
      "      7        1.6387       0.3837        1.6370  0.4508\n",
      "      8        1.6145       0.3869        1.6110  0.4575\n",
      "      9        1.5434       0.3787        1.5608  0.4512\n",
      "     10        1.5182       0.4181        1.5397  0.4508\n",
      "     11        1.4738       0.4050        1.4799  0.5052\n",
      "     12        1.4587       0.4163        1.4906  0.8082\n",
      "     13        1.4714       0.4419        1.5168  0.4748\n",
      "     14        1.4146       0.4106        1.4542  0.5365\n",
      "     15        1.4123       0.4012        1.4193  0.4790\n",
      "     16        1.3825       0.3756        1.4761  0.5207\n",
      "     17        1.3996       0.4200        1.3899  0.5291\n",
      "     18        1.3253       0.4838        1.4020  0.5403\n",
      "     19        1.2380       0.5162        1.2366  0.4727\n",
      "     20        1.1575       0.5481        1.2258  0.5429\n",
      "     21        1.1154       0.5631        1.2159  0.5048\n",
      "     22        1.0541       0.5437        1.1351  0.5015\n",
      "     23        1.0091       0.5887        1.1330  0.4754\n",
      "     24        1.0077       0.6106        1.0966  0.4879\n",
      "     25        0.9422       0.6338        1.0285  0.4773\n",
      "     26        0.9007       0.6500        0.9662  0.5157\n",
      "     27        0.8127       0.6431        0.8442  0.4715\n",
      "     28        0.7455       0.6519        0.8442  0.5669\n",
      "     29        0.7316       0.6650        0.8503  0.4926\n",
      "     30        0.6705       0.7019        0.8191  0.5575\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.3833       0.4806        1.4206  0.4822\n",
      "      2        1.2695       0.5437        1.1928  0.4950\n",
      "      3        1.0661       0.5994        1.1042  0.5167\n",
      "      4        0.9065       0.6300        0.9275  0.5048\n",
      "      5        0.8136       0.6969        0.7982  0.4816\n",
      "      6        0.7275       0.7137        0.7367  0.5700\n",
      "      7        0.6471       0.7456        0.7061  0.5355\n",
      "      8        0.5872       0.7531        0.6843  0.5176\n",
      "      9        0.5564       0.7844        0.6353  0.4951\n",
      "     10        0.5361       0.7444        0.6330  0.4944\n",
      "     11        0.4915       0.7681        0.6210  0.5053\n",
      "     12        0.5051       0.8013        0.6292  0.4613\n",
      "     13        0.4374       0.8000        0.5761  0.5106\n",
      "     14        0.4381       0.7994        0.5842  0.4886\n",
      "     15        0.4174       0.7863        0.6011  0.5105\n",
      "     16        0.4086       0.8031        0.5958  0.4659\n",
      "     17        0.3810       0.7975        0.5857  0.4609\n",
      "     18        0.3698       0.8006        0.5729  0.4438\n",
      "     19        0.3689       0.8025        0.5766  0.4585\n",
      "     20        0.3452       0.7956        0.6264  0.4591\n",
      "     21        0.3551       0.8037        0.6051  0.5122\n",
      "     22        0.3351       0.8044        0.5859  0.4890\n",
      "     23        0.3611       0.7956        0.6232  0.5300\n",
      "     24        0.3671       0.8094        0.5918  0.4708\n",
      "     25        0.3187       0.7937        0.6400  0.4691\n",
      "     26        0.3238       0.8100        0.6395  0.5708\n",
      "     27        0.3308       0.7975        0.6788  0.5888\n",
      "     28        0.3614       0.8206        0.5925  0.5243\n",
      "     29        0.3154       0.7937        0.6871  0.5325\n",
      "     30        0.3062       0.8137        0.6050  0.4737\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.0260       0.3812        1.4833  0.4694\n",
      "      2        1.3726       0.4387        1.3838  0.5250\n",
      "      3        1.2883       0.5494        1.2974  0.4941\n",
      "      4        1.1900       0.5787        1.2042  0.4813\n",
      "      5        1.1342       0.5950        1.1556  0.4702\n",
      "      6        1.0695       0.6331        1.0726  0.4822\n",
      "      7        0.9816       0.6750        0.9997  0.4596\n",
      "      8        0.9390       0.6750        0.9806  0.4557\n",
      "      9        0.8818       0.6769        0.9521  0.4575\n",
      "     10        0.8531       0.7037        0.9491  0.4725\n",
      "     11        0.8556       0.6956        0.9297  0.4674\n",
      "     12        0.8126       0.6994        0.9104  0.4820\n",
      "     13        0.7946       0.7125        0.8995  0.5503\n",
      "     14        0.7797       0.6931        0.8742  0.5025\n",
      "     15        0.7500       0.7094        0.8930  0.5551\n",
      "     16        0.7294       0.7200        0.8522  0.4815\n",
      "     17        0.7404       0.6969        0.9490  0.4987\n",
      "     18        0.7294       0.7163        0.8560  0.4842\n",
      "     19        0.7209       0.7231        0.8443  0.5109\n",
      "     20        0.6824       0.7388        0.8739  0.5609\n",
      "     21        0.6684       0.7225        0.8213  0.4793\n",
      "     22        0.6241       0.7419        0.7802  0.5517\n",
      "     23        0.6262       0.7275        0.8450  0.4734\n",
      "     24        0.6030       0.7494        0.7725  0.4575\n",
      "     25        0.5768       0.7506        0.7680  0.7734\n",
      "     26        0.5643       0.7406        0.7703  0.7204\n",
      "     27        0.5686       0.7575        0.7437  0.5381\n",
      "     28        0.5419       0.7675        0.8327  0.5666\n",
      "     29        0.5647       0.7538        0.7865  0.5755\n",
      "     30        0.5374       0.7500        0.7606  0.5148\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.1493       0.1544        2.2143  0.5889\n",
      "      2        2.0751       0.3194        1.8794  0.5797\n",
      "      3        1.8021       0.3931        1.7000  0.5336\n",
      "      4        1.6495       0.4431        1.6113  0.4937\n",
      "      5        1.5308       0.4587        1.5199  0.5022\n",
      "      6        1.4787       0.4575        1.4979  0.6169\n",
      "      7        1.4369       0.4662        1.4567  0.4859\n",
      "      8        1.4115       0.4569        1.4755  0.4715\n",
      "      9        1.3764       0.4713        1.4177  0.4796\n",
      "     10        1.3559       0.4619        1.4304  0.4654\n",
      "     11        1.3476       0.4637        1.3971  0.4552\n",
      "     12        1.3256       0.4700        1.4157  0.4524\n",
      "     13        1.3130       0.4650        1.3812  0.4633\n",
      "     14        1.3052       0.4719        1.3808  0.4576\n",
      "     15        1.2718       0.4600        1.2925  0.4723\n",
      "     16        1.0854       0.5244        1.1366  0.4541\n",
      "     17        1.0719       0.5863        1.1099  0.5520\n",
      "     18        0.9697       0.6244        1.0456  0.4800\n",
      "     19        0.9273       0.6294        1.0143  0.4532\n",
      "     20        0.8950       0.6519        0.9605  0.4586\n",
      "     21        0.8646       0.6444        0.9686  0.4559\n",
      "     22        0.8447       0.6506        0.9220  0.4691\n",
      "     23        0.7957       0.6669        0.8845  0.4728\n",
      "     24        0.7805       0.6687        0.9307  0.7442\n",
      "     25        0.7657       0.6975        0.8537  0.7042\n",
      "     26        0.7293       0.7206        0.8077  0.5358\n",
      "     27        0.6657       0.7269        0.8052  0.5176\n",
      "     28        0.6455       0.7219        0.7750  0.4704\n",
      "     29        0.6321       0.7306        0.7759  0.5415\n",
      "     30        0.6073       0.7288        0.7790  0.5598\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4995       0.0944        2.2975  0.5101\n",
      "      2        2.2222       0.1963        2.1279  0.4777\n",
      "      3        2.1187       0.1931        2.0906  0.5063\n",
      "      4        2.0832       0.1888        2.0849  0.5556\n",
      "      5        2.0508       0.1925        2.0467  0.5025\n",
      "      6        2.0374       0.1919        2.0346  0.5060\n",
      "      7        2.0086       0.1906        2.0246  0.4930\n",
      "      8        1.9914       0.2037        2.0015  0.4631\n",
      "      9        1.9659       0.2369        1.9615  0.4792\n",
      "     10        1.9281       0.2075        1.9254  0.4537\n",
      "     11        1.8965       0.2019        1.9418  0.4504\n",
      "     12        1.8517       0.2506        1.8834  0.4431\n",
      "     13        1.8188       0.2531        1.8545  0.5206\n",
      "     14        1.7889       0.2662        1.8217  0.4465\n",
      "     15        1.7609       0.2319        1.8043  0.4814\n",
      "     16        1.7503       0.2569        1.7946  0.4542\n",
      "     17        1.7321       0.2575        1.7923  0.4839\n",
      "     18        1.6858       0.2969        1.7372  0.4520\n",
      "     19        1.5227       0.5212        1.2768  0.4524\n",
      "     20        1.2062       0.5519        1.1706  0.5328\n",
      "     21        1.1253       0.5556        1.1539  0.4502\n",
      "     22        1.0783       0.5506        1.1542  0.4486\n",
      "     23        1.0301       0.5763        1.0868  0.4473\n",
      "     24        1.0054       0.5800        1.0745  0.4473\n",
      "     25        0.9995       0.6012        1.0925  0.4579\n",
      "     26        0.9783       0.6056        1.1084  0.4433\n",
      "     27        0.9441       0.5988        1.1181  0.4472\n",
      "     28        0.9307       0.6119        1.0544  0.4470\n",
      "     29        0.9209       0.6019        1.0375  0.4559\n",
      "     30        0.9027       0.6144        1.0222  0.4873\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.9851       0.3919        1.6642  0.5204\n",
      "      2        1.4744       0.5125        1.3077  0.7962\n",
      "      3        1.1058       0.6556        1.0662  0.6100\n",
      "      4        0.9255       0.6844        0.9678  0.4915\n",
      "      5        0.8406       0.7125        0.8403  0.5805\n",
      "      6        0.6375       0.7744        0.6785  0.4860\n",
      "      7        0.5624       0.7863        0.6235  0.4829\n",
      "      8        0.5221       0.7956        0.6459  0.4789\n",
      "      9        0.5097       0.7875        0.6106  0.4824\n",
      "     10        0.4897       0.8019        0.5873  0.4769\n",
      "     11        0.4715       0.7781        0.6306  0.5750\n",
      "     12        0.4705       0.8075        0.5980  0.5290\n",
      "     13        0.4233       0.8069        0.6317  0.4919\n",
      "     14        0.3998       0.8063        0.6132  0.5189\n",
      "     15        0.4054       0.8075        0.5790  0.4615\n",
      "     16        0.4003       0.8131        0.6440  0.4705\n",
      "     17        0.4208       0.8206        0.6087  0.4528\n",
      "     18        0.3583       0.8244        0.6458  0.4593\n",
      "     19        0.3697       0.8169        0.5890  0.5641\n",
      "     20        0.3502       0.8244        0.5797  0.4939\n",
      "     21        0.3428       0.8294        0.6180  0.4774\n",
      "     22        0.3265       0.8244        0.5882  0.5082\n",
      "     23        0.3293       0.8187        0.5773  0.4631\n",
      "     24        0.2969       0.8131        0.6386  0.4690\n",
      "     25        0.3498       0.8106        0.5982  0.4890\n",
      "     26        0.3132       0.8319        0.5985  0.4657\n",
      "     27        0.2975       0.8219        0.5959  0.4946\n",
      "     28        0.2806       0.8244        0.6321  0.4768\n",
      "     29        0.3043       0.8194        0.6873  0.5050\n",
      "     30        0.2897       0.8237        0.5867  0.4797\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6104       0.5262        1.4391  0.4915\n",
      "      2        1.2519       0.6519        1.1139  0.4722\n",
      "      3        0.9357       0.7025        0.9048  0.5702\n",
      "      4        0.7834       0.7113        0.8225  0.6079\n",
      "      5        0.7068       0.7250        0.7735  0.6152\n",
      "      6        0.6371       0.7438        0.7742  0.4963\n",
      "      7        0.5830       0.7769        0.6689  0.4823\n",
      "      8        0.5200       0.7850        0.6592  0.5304\n",
      "      9        0.4655       0.8063        0.5977  0.4676\n",
      "     10        0.4268       0.7856        0.6100  0.4971\n",
      "     11        0.4037       0.7900        0.6057  0.4826\n",
      "     12        0.3840       0.8100        0.5597  0.5040\n",
      "     13        0.3730       0.8000        0.5943  0.4829\n",
      "     14        0.3528       0.8119        0.6366  0.5033\n",
      "     15        0.3377       0.8175        0.5900  0.4893\n",
      "     16        0.3258       0.8231        0.5789  0.4878\n",
      "     17        0.3213       0.8256        0.5628  0.4566\n",
      "     18        0.2907       0.8219        0.5842  0.5100\n",
      "     19        0.2835       0.8256        0.5730  0.4542\n",
      "     20        0.2918       0.8256        0.6417  0.4585\n",
      "     21        0.2826       0.8219        0.6131  0.5592\n",
      "     22        0.3022       0.8150        0.6163  0.5010\n",
      "     23        0.2931       0.8281        0.5854  0.4688\n",
      "     24        0.2550       0.8294        0.5824  0.4705\n",
      "     25        0.2418       0.8263        0.6239  0.4549\n",
      "     26        0.2458       0.8306        0.5874  0.4545\n",
      "     27        0.2488       0.8300        0.6249  0.5390\n",
      "     28        0.2542       0.8287        0.5949  0.4804\n",
      "     29        0.2379       0.8363        0.5826  0.4710\n",
      "     30        0.2257       0.8119        0.7195  0.5978\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6337       0.2950        1.9565  0.5357\n",
      "      2        1.7652       0.4225        1.6267  0.5198\n",
      "      3        1.4130       0.5444        1.3032  0.4803\n",
      "      4        1.1582       0.5675        1.1153  0.5350\n",
      "      5        0.9557       0.6306        0.9423  0.5031\n",
      "      6        0.7876       0.6975        0.8106  0.4941\n",
      "      7        0.6794       0.7150        0.7473  0.4710\n",
      "      8        0.6453       0.6856        0.8261  0.4639\n",
      "      9        0.6128       0.7250        0.7308  0.4503\n",
      "     10        0.6068       0.7250        0.7001  0.4571\n",
      "     11        0.5719       0.7238        0.6984  0.4810\n",
      "     12        0.5611       0.7556        0.6630  0.4722\n",
      "     13        0.5383       0.7594        0.6446  0.4753\n",
      "     14        0.4981       0.7594        0.6760  0.4675\n",
      "     15        0.4809       0.7844        0.6661  0.4839\n",
      "     16        0.4858       0.7781        0.6357  0.4566\n",
      "     17        0.5208       0.7906        0.6270  0.4584\n",
      "     18        0.4815       0.7800        0.6650  0.4466\n",
      "     19        0.4717       0.7987        0.6410  0.4949\n",
      "     20        0.4372       0.7806        0.6311  0.5095\n",
      "     21        0.4142       0.7956        0.6568  0.5040\n",
      "     22        0.3979       0.8081        0.6325  0.4935\n",
      "     23        0.4160       0.8056        0.6266  0.4969\n",
      "     24        0.4052       0.8206        0.6232  0.4963\n",
      "     25        0.4054       0.7963        0.6675  0.4623\n",
      "     26        0.3850       0.8187        0.5861  0.4635\n",
      "     27        0.3599       0.8144        0.6146  0.4767\n",
      "     28        0.3438       0.8200        0.5977  0.4717\n",
      "     29        0.3525       0.8137        0.6168  0.4585\n",
      "     30        0.3328       0.8163        0.6443  0.4595\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.7853       0.4050        1.8242  0.4745\n",
      "      2        1.4489       0.6062        1.1261  0.4738\n",
      "      3        1.0306       0.6375        1.0298  0.4652\n",
      "      4        0.8905       0.6706        0.9062  0.4826\n",
      "      5        0.8200       0.6994        0.8180  0.4726\n",
      "      6        0.7437       0.6925        0.7803  0.4796\n",
      "      7        0.6770       0.7512        0.6963  0.4802\n",
      "      8        0.5827       0.7769        0.6286  0.4701\n",
      "      9        0.5329       0.7875        0.5948  0.4931\n",
      "     10        0.4839       0.7994        0.5672  0.4842\n",
      "     11        0.4886       0.7819        0.6216  0.4744\n",
      "     12        0.4448       0.8206        0.5485  0.4656\n",
      "     13        0.4267       0.8131        0.5809  0.4769\n",
      "     14        0.3975       0.8187        0.5604  0.4735\n",
      "     15        0.3876       0.8263        0.5629  0.4649\n",
      "     16        0.3846       0.8281        0.5367  0.4688\n",
      "     17        0.3835       0.8075        0.6327  0.4697\n",
      "     18        0.3676       0.8106        0.6185  0.4884\n",
      "     19        0.3565       0.8219        0.5552  0.4719\n",
      "     20        0.3325       0.8263        0.5607  0.4548\n",
      "     21        0.3386       0.8094        0.5869  0.4629\n",
      "     22        0.3359       0.8194        0.5717  0.4679\n",
      "     23        0.3299       0.8363        0.5341  0.4715\n",
      "     24        0.3113       0.8350        0.5290  0.4789\n",
      "     25        0.3093       0.8281        0.5662  0.4648\n",
      "     26        0.2923       0.8306        0.5494  0.4839\n",
      "     27        0.2994       0.8244        0.5621  0.4660\n",
      "     28        0.2989       0.8256        0.5831  0.4662\n",
      "     29        0.2766       0.8319        0.5804  0.4753\n",
      "     30        0.2817       0.8119        0.6122  0.4755\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5613       0.3862        1.7772  0.4691\n",
      "      2        1.6726       0.4688        1.4836  0.5023\n",
      "      3        1.2212       0.6394        1.2071  0.4756\n",
      "      4        0.9980       0.6919        0.9226  0.4605\n",
      "      5        0.8870       0.7119        0.9370  0.5259\n",
      "      6        0.8271       0.7362        0.8380  0.4524\n",
      "      7        0.7501       0.7288        0.8118  0.4651\n",
      "      8        0.7071       0.7500        0.8259  0.4781\n",
      "      9        0.7046       0.7544        0.7314  0.4989\n",
      "     10        0.5945       0.7538        0.6979  0.4863\n",
      "     11        0.5777       0.7394        0.6880  0.4573\n",
      "     12        0.5350       0.7712        0.6330  0.4645\n",
      "     13        0.5070       0.7831        0.6051  0.4791\n",
      "     14        0.4777       0.7856        0.6044  0.4638\n",
      "     15        0.4712       0.7963        0.6148  0.4727\n",
      "     16        0.4574       0.7819        0.6546  0.4765\n",
      "     17        0.4409       0.7900        0.6128  0.4682\n",
      "     18        0.4115       0.8031        0.5906  0.4885\n",
      "     19        0.4092       0.7712        0.6537  0.4576\n",
      "     20        0.4319       0.8250        0.5640  0.4555\n",
      "     21        0.3757       0.8231        0.5644  0.4496\n",
      "     22        0.3956       0.7987        0.6594  0.4545\n",
      "     23        0.3774       0.8275        0.5603  0.4708\n",
      "     24        0.3431       0.8281        0.5772  0.4584\n",
      "     25        0.3417       0.8294        0.5659  0.4568\n",
      "     26        0.3343       0.8319        0.5527  0.4556\n",
      "     27        0.3308       0.8400        0.5188  0.4554\n",
      "     28        0.3289       0.8081        0.5883  0.4571\n",
      "     29        0.3275       0.8319        0.5784  0.4467\n",
      "     30        0.3128       0.8300        0.5625  0.4606\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4634       0.5587        1.3783  0.4624\n",
      "      2        1.2707       0.6881        1.2000  0.4584\n",
      "      3        1.0353       0.6987        1.0474  0.4585\n",
      "      4        0.9532       0.7219        0.9979  0.4636\n",
      "      5        0.8725       0.7238        0.9355  0.4490\n",
      "      6        0.8326       0.7056        0.8957  0.4594\n",
      "      7        0.8321       0.7025        0.8647  0.4555\n",
      "      8        0.7443       0.7319        0.8014  0.4618\n",
      "      9        0.6956       0.7188        0.8116  0.4585\n",
      "     10        0.6690       0.7175        0.7721  0.4578\n",
      "     11        0.6016       0.7125        0.7124  0.4546\n",
      "     12        0.5676       0.7338        0.7076  0.4536\n",
      "     13        0.5449       0.7488        0.6762  0.4570\n",
      "     14        0.5247       0.7506        0.6877  0.4561\n",
      "     15        0.4964       0.7381        0.7010  0.4615\n",
      "     16        0.5167       0.7625        0.6331  0.4549\n",
      "     17        0.4679       0.7775        0.6376  0.4564\n",
      "     18        0.4353       0.7863        0.6002  0.4569\n",
      "     19        0.4118       0.8037        0.6019  0.4562\n",
      "     20        0.4078       0.7850        0.6659  0.4605\n",
      "     21        0.4355       0.7944        0.6290  0.4592\n",
      "     22        0.4287       0.8181        0.5853  0.4525\n",
      "     23        0.3869       0.8144        0.6002  0.4529\n",
      "     24        0.3622       0.8263        0.5866  0.4538\n",
      "     25        0.3688       0.8281        0.5676  0.4577\n",
      "     26        0.3534       0.8244        0.5705  0.4652\n",
      "     27        0.3473       0.8331        0.5602  0.4547\n",
      "     28        0.3171       0.8344        0.5637  0.4542\n",
      "     29        0.3049       0.8250        0.5816  0.4552\n",
      "     30        0.3171       0.8256        0.6271  0.4601\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4827       0.1412        2.2287  0.4933\n",
      "      2        2.2462       0.1506        2.2019  0.4816\n",
      "      3        2.1977       0.1656        2.1635  0.4823\n",
      "      4        2.1702       0.1756        2.1411  0.4860\n",
      "      5        2.1538       0.1762        2.2839  0.4827\n",
      "      6        2.1522       0.1706        2.1503  0.4862\n",
      "      7        2.1328       0.1894        2.1430  0.4743\n",
      "      8        2.1169       0.1900        2.1442  0.4801\n",
      "      9        2.0965       0.1913        2.1262  0.4892\n",
      "     10        2.0918       0.1944        2.1438  0.4806\n",
      "     11        2.0851       0.1875        2.0927  0.4809\n",
      "     12        2.0781       0.1938        2.0798  0.4847\n",
      "     13        2.0713       0.1994        2.0743  0.4820\n",
      "     14        2.0617       0.1975        2.0680  0.4870\n",
      "     15        1.9650       0.2469        1.8826  0.5135\n",
      "     16        1.8439       0.1988        1.8578  0.4944\n",
      "     17        1.8309       0.2050        1.8556  0.4899\n",
      "     18        1.7791       0.3650        1.6219  0.4833\n",
      "     19        1.5861       0.4019        1.5967  0.4916\n",
      "     20        1.4844       0.4275        1.4863  0.4886\n",
      "     21        1.4376       0.4050        1.4624  0.4889\n",
      "     22        1.4092       0.4350        1.4329  0.4852\n",
      "     23        1.4197       0.4387        1.4258  0.4846\n",
      "     24        1.3670       0.4581        1.3806  0.4908\n",
      "     25        1.3306       0.4512        1.3674  0.4958\n",
      "     26        1.2971       0.4694        1.3398  0.4888\n",
      "     27        1.2826       0.4550        1.4290  0.4882\n",
      "     28        1.2793       0.4744        1.3172  0.4859\n",
      "     29        1.2486       0.4781        1.3152  0.4872\n",
      "     30        1.2412       0.4744        1.3487  0.4939\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.1779       0.3044        1.8789  0.4919\n",
      "      2        1.6538       0.4431        1.6062  0.4878\n",
      "      3        1.5009       0.4400        1.5383  0.5040\n",
      "      4        1.4445       0.4569        1.5159  0.4832\n",
      "      5        1.4070       0.4531        1.4458  0.4886\n",
      "      6        1.3666       0.4550        1.4117  0.4863\n",
      "      7        1.3403       0.4637        1.4293  0.4837\n",
      "      8        1.3269       0.4606        1.5212  0.4896\n",
      "      9        1.3106       0.4625        1.3826  0.4855\n",
      "     10        1.2948       0.4637        1.3820  0.4835\n",
      "     11        1.2718       0.4650        1.3866  0.4947\n",
      "     12        1.2662       0.4669        1.4013  0.4825\n",
      "     13        1.2611       0.4688        1.4312  0.4892\n",
      "     14        1.2593       0.4644        1.3462  0.4859\n",
      "     15        1.2421       0.4575        1.3446  0.5584\n",
      "     16        1.2454       0.4688        1.3644  0.5428\n",
      "     17        1.2357       0.4650        1.3453  0.5296\n",
      "     18        1.2105       0.4694        1.3359  0.5317\n",
      "     19        1.1632       0.5631        1.2111  0.5410\n",
      "     20        1.0541       0.5700        1.1635  0.5629\n",
      "     21        0.9934       0.5687        1.1079  0.5080\n",
      "     22        0.9632       0.5656        1.1936  0.4949\n",
      "     23        1.0103       0.5669        1.1168  0.5046\n",
      "     24        0.9504       0.5994        1.1876  0.5195\n",
      "     25        0.9262       0.6044        1.0631  0.5075\n",
      "     26        0.8906       0.6044        1.0033  0.5139\n",
      "     27        0.8859       0.6094        1.0599  0.4901\n",
      "     28        0.9049       0.6056        1.0816  0.4941\n",
      "     29        0.8728       0.6144        1.0370  0.4879\n",
      "     30        0.8409       0.6150        0.9945  0.4910\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.9291       0.2188        2.0350  0.4969\n",
      "      2        1.9030       0.3312        1.8304  0.4871\n",
      "      3        1.6922       0.4000        1.6284  0.4975\n",
      "      4        1.5679       0.3881        1.5837  0.4992\n",
      "      5        1.4642       0.4662        1.5103  0.4863\n",
      "      6        1.4131       0.4706        1.4321  0.5015\n",
      "      7        1.3522       0.5437        1.3442  0.5308\n",
      "      8        1.2163       0.5881        1.2139  0.5322\n",
      "      9        1.1256       0.6188        1.1977  0.5958\n",
      "     10        1.0419       0.6162        1.1183  0.4929\n",
      "     11        0.9861       0.6531        1.0519  0.5484\n",
      "     12        0.9283       0.7019        1.0178  0.5293\n",
      "     13        0.8735       0.6800        0.9758  0.5169\n",
      "     14        0.8625       0.7006        0.9457  0.5094\n",
      "     15        0.8335       0.7081        0.9285  0.5037\n",
      "     16        0.7970       0.6975        0.9577  0.4876\n",
      "     17        0.8043       0.7037        0.9183  0.4809\n",
      "     18        0.7491       0.7019        0.9377  0.4889\n",
      "     19        0.7398       0.7169        0.8809  0.4849\n",
      "     20        0.7274       0.7000        0.8904  0.4897\n",
      "     21        0.7099       0.7081        0.9012  0.4948\n",
      "     22        0.6888       0.7238        0.8669  0.4842\n",
      "     23        0.6653       0.7281        0.8739  0.4854\n",
      "     24        0.6723       0.7106        0.8682  0.4927\n",
      "     25        0.6507       0.7406        0.8464  0.4899\n",
      "     26        0.6332       0.7375        0.8257  0.4931\n",
      "     27        0.6155       0.7262        0.8634  0.4976\n",
      "     28        0.6094       0.7225        0.8643  0.5003\n",
      "     29        0.6241       0.7281        0.9019  0.4923\n",
      "     30        0.5814       0.7431        0.7779  0.4928\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6872       0.1900        2.1313  0.5099\n",
      "      2        2.1107       0.1969        2.0771  0.4975\n",
      "      3        2.0793       0.1969        2.0701  0.4864\n",
      "      4        2.0732       0.1906        2.0792  0.4935\n",
      "      5        2.0746       0.1975        2.0533  0.4866\n",
      "      6        2.0691       0.1975        2.0498  0.4975\n",
      "      7        2.0500       0.1981        2.0451  0.4976\n",
      "      8        2.0498       0.1956        2.0493  0.4886\n",
      "      9        2.0418       0.1975        2.0411  0.5040\n",
      "     10        2.0378       0.1994        2.0364  0.4897\n",
      "     11        2.0352       0.1963        2.0426  0.4893\n",
      "     12        2.0363       0.1981        2.0344  0.5013\n",
      "     13        2.0315       0.2006        2.0301  0.4892\n",
      "     14        2.0287       0.1994        2.0281  0.5015\n",
      "     15        2.0280       0.2000        2.0386  0.4944\n",
      "     16        2.0292       0.1956        2.0336  0.5010\n",
      "     17        2.0248       0.1994        2.0281  0.4896\n",
      "     18        2.0276       0.1994        2.0215  0.5277\n",
      "     19        2.0195       0.2031        2.0287  0.5075\n",
      "     20        2.0129       0.2106        2.0113  0.5086\n",
      "     21        2.0150       0.2087        2.0303  0.5207\n",
      "     22        2.0098       0.2231        1.9820  0.5208\n",
      "     23        1.8646       0.3081        1.7429  0.5024\n",
      "     24        1.6336       0.3775        1.7079  0.5007\n",
      "     25        1.6358       0.3244        1.6024  0.5041\n",
      "     26        1.4953       0.3787        1.4873  0.5029\n",
      "     27        1.4395       0.4150        1.4436  0.5156\n",
      "     28        1.3868       0.4756        1.4151  0.4889\n",
      "     29        1.3416       0.4313        1.4210  0.5151\n",
      "     30        1.3209       0.4906        1.3593  0.5060\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.0788       0.1025        2.3171  0.5095\n",
      "      2        2.2937       0.1394        2.2113  0.4972\n",
      "      3        2.2150       0.1819        2.1830  0.5068\n",
      "      4        2.1485       0.1975        2.1053  0.4896\n",
      "      5        2.0798       0.2087        2.0731  0.4901\n",
      "      6        2.0460       0.2025        2.0834  0.5088\n",
      "      7        2.0426       0.2062        2.0297  0.5095\n",
      "      8        2.0114       0.2006        2.0408  0.4933\n",
      "      9        2.0019       0.2087        2.0063  0.5183\n",
      "     10        1.9795       0.2013        1.9882  0.4867\n",
      "     11        1.9523       0.2244        1.9633  0.4911\n",
      "     12        1.8945       0.2238        1.8942  0.5099\n",
      "     13        1.8283       0.2069        1.8556  0.4982\n",
      "     14        1.8038       0.2150        1.8675  0.5040\n",
      "     15        1.7854       0.2775        1.8458  0.4958\n",
      "     16        1.7769       0.2456        1.8235  0.5012\n",
      "     17        1.7411       0.2781        1.7888  0.4978\n",
      "     18        1.7304       0.2112        1.7795  0.4945\n",
      "     19        1.7120       0.2431        1.7586  0.5006\n",
      "     20        1.7013       0.2419        1.7748  0.4969\n",
      "     21        1.6944       0.3106        1.7791  0.4884\n",
      "     22        1.6825       0.2150        1.7898  0.5031\n",
      "     23        1.6702       0.2531        1.7319  0.5579\n",
      "     24        1.6614       0.3331        1.7494  0.5135\n",
      "     25        1.6449       0.3319        1.7769  0.5040\n",
      "     26        1.6347       0.2944        1.7805  0.5140\n",
      "     27        1.6254       0.3556        1.7695  0.5024\n",
      "     28        1.5755       0.2950        1.6760  0.5491\n",
      "     29        1.4949       0.4288        1.5847  0.4855\n",
      "     30        1.4076       0.4238        1.4915  0.4840\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.2038       0.2494        1.8429  0.5975\n",
      "      2        1.8028       0.3500        1.7665  0.5254\n",
      "      3        1.7429       0.3581        1.6728  0.5402\n",
      "      4        1.6083       0.4325        1.5780  0.5068\n",
      "      5        1.5172       0.4431        1.4962  0.4956\n",
      "      6        1.4132       0.5131        1.3425  0.4868\n",
      "      7        1.2256       0.5800        1.1965  0.5325\n",
      "      8        1.0728       0.6225        1.1076  0.5075\n",
      "      9        0.9694       0.6569        0.9951  0.4914\n",
      "     10        0.9226       0.6844        0.9387  0.4922\n",
      "     11        0.8577       0.7075        0.9324  0.4881\n",
      "     12        0.8042       0.7163        0.8753  0.4952\n",
      "     13        0.7499       0.7144        0.8236  0.4919\n",
      "     14        0.7116       0.7331        0.8510  0.5045\n",
      "     15        0.6362       0.7375        0.6560  0.5186\n",
      "     16        0.5415       0.7369        0.6396  0.5045\n",
      "     17        0.5721       0.7388        0.6557  0.5000\n",
      "     18        0.5215       0.7519        0.6215  0.5006\n",
      "     19        0.5010       0.7550        0.6319  0.5029\n",
      "     20        0.4864       0.7581        0.6295  0.5386\n",
      "     21        0.4497       0.7719        0.6056  0.5674\n",
      "     22        0.4494       0.7731        0.6031  1.0372\n",
      "     23        0.4302       0.7869        0.6176  0.6270\n",
      "     24        0.4262       0.7925        0.5977  0.5308\n",
      "     25        0.4388       0.7812        0.6585  0.5111\n",
      "     26        0.4043       0.8006        0.6042  0.4872\n",
      "     27        0.4120       0.7712        0.6999  0.5032\n",
      "     28        0.3958       0.7987        0.6347  0.5148\n",
      "     29        0.3793       0.7950        0.6197  0.5741\n",
      "     30        0.3823       0.7856        0.6537  0.6650\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.9350       0.4244        1.7436  0.6366\n",
      "      2        1.5567       0.5763        1.4553  0.6067\n",
      "      3        1.3410       0.5994        1.3161  0.5192\n",
      "      4        1.1853       0.6219        1.2040  0.5138\n",
      "      5        1.0394       0.6331        1.2195  0.5586\n",
      "      6        0.9252       0.7025        0.9831  0.6618\n",
      "      7        0.8286       0.7262        0.8895  0.5228\n",
      "      8        0.7813       0.7281        0.8996  0.5438\n",
      "      9        0.7599       0.7288        0.9451  0.5254\n",
      "     10        0.7151       0.7438        0.8328  0.5708\n",
      "     11        0.7139       0.7362        0.8496  0.5625\n",
      "     12        0.6753       0.7431        0.7756  0.5468\n",
      "     13        0.5315       0.7562        0.6938  0.5278\n",
      "     14        0.4930       0.7512        0.6468  0.5470\n",
      "     15        0.4610       0.7812        0.6336  0.5237\n",
      "     16        0.4710       0.7800        0.6259  0.5358\n",
      "     17        0.4476       0.7937        0.6147  0.5290\n",
      "     18        0.4476       0.7581        0.6841  0.5343\n",
      "     19        0.4234       0.8050        0.5729  0.5236\n",
      "     20        0.4042       0.7844        0.6622  0.5421\n",
      "     21        0.4001       0.7969        0.6104  0.6916\n",
      "     22        0.3628       0.8094        0.5868  0.6529\n",
      "     23        0.3569       0.8181        0.5651  0.6385\n",
      "     24        0.3727       0.8137        0.6083  0.4908\n",
      "     25        0.3289       0.8337        0.5558  0.4982\n",
      "     26        0.3405       0.8294        0.5494  0.5212\n",
      "     27        0.3206       0.8256        0.5679  0.5046\n",
      "     28        0.3081       0.8337        0.5628  0.5464\n",
      "     29        0.3093       0.8281        0.5637  0.5632\n",
      "     30        0.3092       0.8219        0.6577  0.5826\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6180       0.4256        1.6152  0.5516\n",
      "      2        1.4380       0.5400        1.3279  0.7642\n",
      "      3        1.2398       0.5950        1.1925  0.9594\n",
      "      4        1.1090       0.6106        1.0953  0.6071\n",
      "      5        0.9617       0.6406        0.9929  0.5664\n",
      "      6        0.8586       0.7075        0.8307  0.5201\n",
      "      7        0.8040       0.7050        0.8437  0.5937\n",
      "      8        0.7472       0.7087        0.8624  0.5949\n",
      "      9        0.7955       0.7156        0.8697  0.5434\n",
      "     10        0.6968       0.7338        0.7771  0.4941\n",
      "     11        0.6930       0.6975        0.8561  0.5816\n",
      "     12        0.6674       0.7431        0.7575  0.9255\n",
      "     13        0.6170       0.7744        0.7065  0.5996\n",
      "     14        0.6235       0.7631        0.7554  0.5415\n",
      "     15        0.5802       0.7556        0.7383  0.5065\n",
      "     16        0.5573       0.7719        0.7147  0.5019\n",
      "     17        0.5244       0.7462        0.7392  0.5217\n",
      "     18        0.4948       0.7700        0.6353  0.5128\n",
      "     19        0.4958       0.7762        0.6347  0.5294\n",
      "     20        0.4603       0.7875        0.6064  0.5088\n",
      "     21        0.4423       0.7863        0.6219  0.5096\n",
      "     22        0.4304       0.7925        0.5957  0.5532\n",
      "     23        0.4328       0.7937        0.5731  0.7091\n",
      "     24        0.4128       0.7950        0.5946  0.6098\n",
      "     25        0.4046       0.8044        0.5751  0.5863\n",
      "     26        0.3958       0.8087        0.5537  0.6904\n",
      "     27        0.3977       0.8219        0.5669  0.5553\n",
      "     28        0.3804       0.7969        0.5676  0.6178\n",
      "     29        0.3591       0.8213        0.5562  0.5708\n",
      "     30        0.3797       0.8219        0.5729  0.5150\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5476       0.0862        2.1437  0.5682\n",
      "      2        2.0890       0.1781        2.0447  0.5191\n",
      "      3        2.0296       0.1781        2.0316  0.5398\n",
      "      4        2.0143       0.1862        2.0151  0.6423\n",
      "      5        1.9856       0.1556        1.9972  0.7006\n",
      "      6        1.9631       0.1787        1.9649  0.5498\n",
      "      7        1.9470       0.2137        1.9353  0.5100\n",
      "      8        1.9007       0.2406        1.8713  0.5430\n",
      "      9        1.8455       0.2194        1.8444  0.5299\n",
      "     10        1.8153       0.2313        1.8111  0.5457\n",
      "     11        1.7940       0.2487        1.7936  0.7024\n",
      "     12        1.7704       0.2537        1.7840  0.6448\n",
      "     13        1.7746       0.2869        1.7983  0.6201\n",
      "     14        1.7446       0.2431        1.7570  0.7820\n",
      "     15        1.7224       0.2656        1.7328  0.7545\n",
      "     16        1.7029       0.2725        1.7377  0.6460\n",
      "     17        1.7364       0.2737        1.7323  0.5513\n",
      "     18        1.7049       0.2706        1.7070  0.5062\n",
      "     19        1.5855       0.3681        1.5079  0.5034\n",
      "     20        1.4990       0.3400        1.5090  0.5201\n",
      "     21        1.4427       0.3769        1.4691  0.5090\n",
      "     22        1.3785       0.4719        1.3761  0.5841\n",
      "     23        1.2778       0.5244        1.2651  0.5668\n",
      "     24        1.1977       0.5219        1.2046  0.7046\n",
      "     25        1.1342       0.5419        1.1299  0.6321\n",
      "     26        1.1199       0.5494        1.1666  0.5916\n",
      "     27        1.0538       0.5500        1.0856  0.5513\n",
      "     28        1.0122       0.5550        1.0933  0.5235\n",
      "     29        0.9788       0.5769        1.0397  0.5927\n",
      "     30        0.9364       0.5975        1.0401  0.5530\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.1509       0.2894        1.8750  0.5345\n",
      "      2        1.7372       0.3488        1.6075  0.5286\n",
      "      3        1.4713       0.4800        1.3844  0.5730\n",
      "      4        1.2704       0.5494        1.2713  0.5126\n",
      "      5        1.1820       0.5581        1.1952  0.5606\n",
      "      6        1.1310       0.5450        1.1542  0.5197\n",
      "      7        1.1045       0.5381        1.1257  0.5410\n",
      "      8        1.0666       0.5375        1.1686  0.5149\n",
      "      9        1.0451       0.5437        1.0761  0.5197\n",
      "     10        0.9971       0.5919        1.0529  0.4891\n",
      "     11        0.9581       0.5875        1.0236  0.4916\n",
      "     12        0.8957       0.6625        0.9289  0.4935\n",
      "     13        0.8101       0.6450        0.9251  0.4954\n",
      "     14        0.7599       0.7269        0.8733  0.4916\n",
      "     15        0.7156       0.7288        0.8285  0.4887\n",
      "     16        0.6965       0.7238        0.8391  0.4914\n",
      "     17        0.6926       0.7094        0.8303  0.4919\n",
      "     18        0.6795       0.7212        0.7995  0.4928\n",
      "     19        0.6449       0.7225        0.8294  0.4923\n",
      "     20        0.6233       0.7319        0.7650  0.4931\n",
      "     21        0.6313       0.7356        0.8693  0.4918\n",
      "     22        0.6145       0.7406        0.8503  0.4979\n",
      "     23        0.5992       0.7344        0.7634  0.4935\n",
      "     24        0.5559       0.7412        0.8117  0.4874\n",
      "     25        0.5569       0.7462        0.7262  0.4939\n",
      "     26        0.4993       0.7688        0.6932  0.5574\n",
      "     27        0.4830       0.7675        0.7597  0.5972\n",
      "     28        0.4805       0.7812        0.6579  0.5333\n",
      "     29        0.4585       0.7819        0.6660  0.5199\n",
      "     30        0.4408       0.7875        0.6619  0.6146\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4200       0.3456        1.7793  0.5914\n",
      "      2        1.6870       0.4419        1.5161  0.5167\n",
      "      3        1.3866       0.5319        1.2136  0.5030\n",
      "      4        1.1176       0.5494        1.0898  0.4928\n",
      "      5        0.9309       0.6525        0.9472  0.4890\n",
      "      6        0.8377       0.7119        0.7681  0.5108\n",
      "      7        0.6952       0.7231        0.7179  0.5296\n",
      "      8        0.6020       0.7669        0.6281  0.4985\n",
      "      9        0.5612       0.7738        0.6597  0.5255\n",
      "     10        0.5441       0.7794        0.5816  0.4920\n",
      "     11        0.4913       0.7819        0.6124  0.5122\n",
      "     12        0.4667       0.8013        0.5471  0.5646\n",
      "     13        0.4317       0.7937        0.5776  0.4971\n",
      "     14        0.4180       0.7913        0.5889  0.5176\n",
      "     15        0.3959       0.8119        0.5671  0.4912\n",
      "     16        0.3739       0.8200        0.5296  0.5098\n",
      "     17        0.3506       0.8269        0.4964  0.5047\n",
      "     18        0.3372       0.8156        0.5498  0.5011\n",
      "     19        0.3260       0.8237        0.5661  0.4939\n",
      "     20        0.3550       0.7956        0.5917  0.4914\n",
      "     21        0.3615       0.8313        0.5339  0.5247\n",
      "     22        0.3127       0.8244        0.5185  0.4936\n",
      "     23        0.3061       0.8313        0.5369  0.4862\n",
      "     24        0.2992       0.8269        0.5462  0.5222\n",
      "     25        0.3086       0.8319        0.5631  0.5206\n",
      "     26        0.2704       0.8281        0.5048  0.4995\n",
      "     27        0.2632       0.8450        0.5069  0.5018\n",
      "     28        0.2761       0.8331        0.5223  0.4997\n",
      "     29        0.2631       0.8287        0.5549  0.4999\n",
      "     30        0.2486       0.8387        0.5331  0.5031\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5030       0.6881        0.9588  0.5187\n",
      "      2        0.7936       0.7456        0.7477  0.5225\n",
      "      3        0.6403       0.7856        0.6342  0.5983\n",
      "      4        0.5990       0.7913        0.6201  0.5190\n",
      "      5        0.5264       0.8206        0.5549  0.5156\n",
      "      6        0.4713       0.8063        0.5916  0.5031\n",
      "      7        0.4829       0.8013        0.5852  0.5134\n",
      "      8        0.4450       0.8206        0.5352  0.4855\n",
      "      9        0.4180       0.8325        0.4969  0.5147\n",
      "     10        0.3666       0.8269        0.4965  0.5019\n",
      "     11        0.3594       0.8194        0.5630  0.4939\n",
      "     12        0.3481       0.8375        0.4965  0.4983\n",
      "     13        0.3275       0.8306        0.5048  0.5055\n",
      "     14        0.3035       0.8419        0.5002  0.5072\n",
      "     15        0.3172       0.8387        0.4976  0.4979\n",
      "     16        0.3099       0.8381        0.5125  0.4950\n",
      "     17        0.3169       0.8406        0.5093  0.4980\n",
      "     18        0.2914       0.8381        0.4923  0.5006\n",
      "     19        0.2653       0.8475        0.4893  0.5451\n",
      "     20        0.2587       0.8319        0.5417  0.5006\n",
      "     21        0.2478       0.8350        0.4994  0.5474\n",
      "     22        0.2557       0.8194        0.6036  0.4994\n",
      "     23        0.2724       0.8381        0.5254  0.4985\n",
      "     24        0.2389       0.8419        0.5018  0.4999\n",
      "     25        0.2381       0.8319        0.5821  0.4939\n",
      "     26        0.2324       0.8375        0.5615  0.5038\n",
      "     27        0.2351       0.8469        0.4830  0.5218\n",
      "     28        0.2485       0.8231        0.5157  0.5013\n",
      "     29        0.2249       0.8375        0.5318  0.5244\n",
      "     30        0.2220       0.8500        0.5277  0.4978\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.2841       0.7106        0.8847  0.5270\n",
      "      2        0.7152       0.7600        0.6721  0.5218\n",
      "      3        0.5961       0.7700        0.6771  0.5057\n",
      "      4        0.5493       0.8125        0.5752  0.5023\n",
      "      5        0.4810       0.7994        0.5831  0.5143\n",
      "      6        0.4778       0.7750        0.6138  0.5128\n",
      "      7        0.4475       0.8169        0.5500  0.5079\n",
      "      8        0.4147       0.7944        0.6163  0.5050\n",
      "      9        0.4085       0.8100        0.5612  0.4941\n",
      "     10        0.3636       0.8206        0.5428  0.5177\n",
      "     11        0.3742       0.8363        0.5050  0.4986\n",
      "     12        0.3430       0.8350        0.4966  0.5051\n",
      "     13        0.3589       0.8381        0.5012  0.4922\n",
      "     14        0.3331       0.8331        0.5251  0.5043\n",
      "     15        0.3403       0.8237        0.5257  0.4962\n",
      "     16        0.3154       0.8375        0.4854  0.5178\n",
      "     17        0.2811       0.8231        0.5485  0.5399\n",
      "     18        0.2847       0.8406        0.5154  0.4996\n",
      "     19        0.2826       0.8056        0.6517  0.5164\n",
      "     20        0.3035       0.8438        0.5227  0.5138\n",
      "     21        0.2567       0.8300        0.5642  0.5596\n",
      "     22        0.2641       0.8231        0.5818  0.5040\n",
      "     23        0.2556       0.8444        0.5476  0.5064\n",
      "     24        0.2441       0.8331        0.5773  0.5162\n",
      "     25        0.2574       0.8263        0.5400  0.5138\n",
      "     26        0.2474       0.8206        0.6003  0.4969\n",
      "     27        0.2505       0.8331        0.6015  0.5338\n",
      "     28        0.2430       0.8294        0.6925  0.4985\n",
      "     29        0.2485       0.8313        0.5860  0.5238\n",
      "     30        0.2438       0.8438        0.5357  0.4922\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        1.7814       0.7063        0.9496  0.5069\n",
      "      2        0.7472       0.7819        0.7131  0.5032\n",
      "      3        0.6239       0.7812        0.6692  0.5252\n",
      "      4        0.5656       0.7975        0.6410  0.5700\n",
      "      5        0.5313       0.7762        0.6494  0.9790\n",
      "      6        0.4965       0.7775        0.7279  0.5933\n",
      "      7        0.4903       0.7875        0.6707  0.5304\n",
      "      8        0.4725       0.8175        0.5887  0.5228\n",
      "      9        0.4191       0.8075        0.5837  0.5139\n",
      "     10        0.4010       0.8187        0.5530  0.5128\n",
      "     11        0.3852       0.8213        0.5499  0.5283\n",
      "     12        0.3747       0.8256        0.5778  0.5642\n",
      "     13        0.3625       0.8306        0.5353  0.6148\n",
      "     14        0.3481       0.8044        0.6221  0.6080\n",
      "     15        0.3465       0.8056        0.5811  0.5460\n",
      "     16        0.3306       0.8325        0.5509  0.5607\n",
      "     17        0.3282       0.8263        0.5756  0.5258\n",
      "     18        0.3135       0.8381        0.5197  0.5900\n",
      "     19        0.2914       0.8256        0.5662  0.6022\n",
      "     20        0.3034       0.8306        0.6015  0.5298\n",
      "     21        0.2648       0.8375        0.5482  0.5588\n",
      "     22        0.2641       0.8344        0.5481  0.5220\n",
      "     23        0.2812       0.8344        0.5951  0.5410\n",
      "     24        0.2798       0.8287        0.6337  0.5403\n",
      "     25        0.2789       0.8344        0.5995  0.5350\n",
      "     26        0.2704       0.8325        0.5735  0.5310\n",
      "     27        0.2557       0.8319        0.5917  0.5093\n",
      "     28        0.2424       0.8313        0.5751  0.5132\n",
      "     29        0.2244       0.8444        0.5912  0.5135\n",
      "     30        0.2272       0.8406        0.5901  0.5115\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.0583       0.2044        1.8822  0.5528\n",
      "      2        1.7910       0.2550        1.6929  0.5231\n",
      "      3        1.6540       0.3400        1.6543  0.5209\n",
      "      4        1.5307       0.4206        1.4838  0.5077\n",
      "      5        1.3677       0.5262        1.2934  0.5079\n",
      "      6        1.2152       0.5581        1.1754  0.5050\n",
      "      7        1.1352       0.6106        1.1222  0.5254\n",
      "      8        1.0373       0.6775        0.9988  0.5172\n",
      "      9        0.9207       0.6687        0.9714  0.5048\n",
      "     10        0.8743       0.6763        0.9488  0.4936\n",
      "     11        0.8406       0.6875        0.9283  0.5068\n",
      "     12        0.7494       0.7206        0.7958  0.5391\n",
      "     13        0.6423       0.7225        0.7279  0.5195\n",
      "     14        0.6161       0.7212        0.7425  0.5323\n",
      "     15        0.5832       0.7200        0.7423  0.5164\n",
      "     16        0.5728       0.7394        0.7250  0.5358\n",
      "     17        0.5467       0.7438        0.7121  0.5253\n",
      "     18        0.5412       0.7412        0.7428  0.5162\n",
      "     19        0.5448       0.7344        0.7753  0.4926\n",
      "     20        0.5268       0.7400        0.7164  0.4852\n",
      "     21        0.5151       0.7350        0.7398  0.4919\n",
      "     22        0.5329       0.7594        0.7288  0.4875\n",
      "     23        0.4838       0.7544        0.7265  0.4890\n",
      "     24        0.4692       0.7544        0.6880  0.4923\n",
      "     25        0.4874       0.7350        0.7965  0.4897\n",
      "     26        0.4615       0.7538        0.7128  0.4893\n",
      "     27        0.4584       0.7519        0.7225  0.4923\n",
      "     28        0.4513       0.7588        0.7153  0.5471\n",
      "     29        0.4457       0.7769        0.7072  0.6391\n",
      "     30        0.4387       0.7806        0.6734  0.5256\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6241       0.1025        2.3109  0.5405\n",
      "      2        2.3101       0.1025        2.3091  0.5885\n",
      "      3        2.3083       0.1025        2.3074  0.5725\n",
      "      4        2.3069       0.1025        2.3062  0.5107\n",
      "      5        2.3058       0.1025        2.3050  0.6047\n",
      "      6        2.3048       0.1025        2.3042  0.6058\n",
      "      7        2.3042       0.1025        2.3038  0.5591\n",
      "      8        2.3038       0.1025        2.3034  0.5306\n",
      "      9        2.3034       0.1025        2.3031  0.5382\n",
      "     10        2.3031       0.1025        2.3028  0.5047\n",
      "     11        2.3029       0.1025        2.3027  0.5194\n",
      "     12        2.3027       0.1025        2.3026  0.5227\n",
      "     13        2.3026       0.1025        2.3025  0.5152\n",
      "     14        2.3025       0.1025        2.3024  0.5111\n",
      "     15        2.3025       0.1025        2.3024  0.5614\n",
      "     16        2.3024       0.1025        2.3023  0.5759\n",
      "     17        2.3024       0.1025        2.3023  0.5479\n",
      "     18        2.3024       0.1025        2.3023  0.5394\n",
      "     19        2.3024       0.1025        2.3023  0.5302\n",
      "     20        2.3024       0.1025        2.3023  0.5603\n",
      "     21        2.3023       0.1025        2.3023  0.5545\n",
      "     22        2.3024       0.1025        2.3023  0.5619\n",
      "     23        2.3023       0.1025        2.3023  0.5870\n",
      "     24        2.3023       0.1025        2.3023  0.5749\n",
      "     25        2.3023       0.1025        2.3023  0.5452\n",
      "     26        2.3023       0.1025        2.3023  0.5284\n",
      "     27        2.3023       0.1025        2.3023  0.5254\n",
      "     28        2.3023       0.1025        2.3023  0.5444\n",
      "     29        2.3023       0.1025        2.3023  0.5093\n",
      "     30        2.3023       0.1025        2.3023  0.5035\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6253       0.2375        2.1466  0.5143\n",
      "      2        2.0423       0.2787        1.9850  0.5116\n",
      "      3        1.9682       0.2781        1.9297  0.5095\n",
      "      4        1.8666       0.2906        1.8452  0.5282\n",
      "      5        1.8209       0.3212        1.8504  0.5188\n",
      "      6        1.7305       0.3638        1.6932  0.5271\n",
      "      7        1.6354       0.3594        1.6351  0.5132\n",
      "      8        1.5998       0.3738        1.5716  0.5217\n",
      "      9        1.5402       0.3506        1.5959  0.6067\n",
      "     10        1.5460       0.3937        1.5170  0.5356\n",
      "     11        1.4148       0.4594        1.4668  0.5391\n",
      "     12        1.3352       0.4869        1.4703  0.5361\n",
      "     13        1.2921       0.4969        1.3276  0.5369\n",
      "     14        1.2524       0.5075        1.3691  0.5242\n",
      "     15        1.2380       0.5075        1.3263  0.5433\n",
      "     16        1.1938       0.5275        1.3213  0.5946\n",
      "     17        1.1885       0.5000        1.2703  0.5744\n",
      "     18        1.1720       0.5387        1.2403  0.5475\n",
      "     19        1.1297       0.5569        1.2700  0.5222\n",
      "     20        1.1036       0.5525        1.2038  0.5956\n",
      "     21        1.0816       0.5544        1.1904  0.5541\n",
      "     22        1.0659       0.5606        1.1469  0.5657\n",
      "     23        1.0576       0.5531        1.1342  0.5659\n",
      "     24        0.9885       0.6281        1.0456  0.6267\n",
      "     25        0.9021       0.6238        1.0288  0.7265\n",
      "     26        0.8749       0.6456        1.0356  0.7001\n",
      "     27        0.8439       0.6687        0.9653  0.5671\n",
      "     28        0.8069       0.6869        0.9561  0.5442\n",
      "     29        0.7792       0.6875        0.9196  0.5351\n",
      "     30        0.7582       0.6800        0.9307  0.5381\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.2088       0.1025        2.3199  0.5826\n",
      "      2        2.3178       0.1044        2.3145  0.5805\n",
      "      3        2.3044       0.1181        2.3170  0.6172\n",
      "      4        2.2907       0.1225        2.2718  0.6413\n",
      "      5        2.2670       0.1325        2.2582  0.5887\n",
      "      6        2.2432       0.1450        2.2224  0.6123\n",
      "      7        2.2032       0.1606        2.1938  0.6385\n",
      "      8        2.1802       0.1688        2.1536  0.6132\n",
      "      9        2.1655       0.1762        2.1375  0.8024\n",
      "     10        2.1295       0.1888        2.1179  0.7996\n",
      "     11        2.1446       0.1844        2.1355  0.7548\n",
      "     12        2.1499       0.1794        2.1170  0.6143\n",
      "     13        2.1040       0.1850        2.0917  0.6246\n",
      "     14        2.1235       0.1769        2.1098  0.7322\n",
      "     15        2.0931       0.1913        2.1665  0.5877\n",
      "     16        2.1017       0.1794        2.1002  0.5862\n",
      "     17        2.0943       0.1900        2.0700  0.6249\n",
      "     18        2.0766       0.1925        2.0671  0.6018\n",
      "     19        2.0750       0.1913        2.0709  0.6009\n",
      "     20        2.0672       0.1906        2.0486  0.5940\n",
      "     21        2.0580       0.1950        2.0616  0.5613\n",
      "     22        2.0667       0.1931        2.1027  0.5417\n",
      "     23        2.0555       0.1938        2.0989  0.6347\n",
      "     24        2.0507       0.1850        2.0470  0.6133\n",
      "     25        2.0492       0.1963        2.1102  0.5956\n",
      "     26        2.0458       0.1931        2.0552  0.5713\n",
      "     27        2.0406       0.1888        2.0561  0.5718\n",
      "     28        2.0116       0.2425        2.0084  0.5707\n",
      "     29        1.9474       0.2806        1.8906  0.5781\n",
      "     30        1.8739       0.2812        1.8563  0.5715\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.2638       0.3469        1.7469  0.6160\n",
      "      2        1.6415       0.3644        1.5669  0.5921\n",
      "      3        1.4613       0.5094        1.3866  0.5878\n",
      "      4        1.3617       0.5337        1.2800  0.6311\n",
      "      5        1.2609       0.5444        1.2323  0.5764\n",
      "      6        1.1974       0.5394        1.2140  0.6461\n",
      "      7        1.1392       0.5563        1.1610  0.6977\n",
      "      8        1.0880       0.5819        1.1096  0.6158\n",
      "      9        1.0362       0.5881        1.1012  0.5861\n",
      "     10        1.0113       0.5825        1.0836  0.5507\n",
      "     11        0.9907       0.5887        1.0830  0.5534\n",
      "     12        0.9413       0.6000        1.0208  0.5157\n",
      "     13        0.9086       0.5881        1.0149  0.5146\n",
      "     14        0.8914       0.5931        0.9908  0.5132\n",
      "     15        0.9052       0.6194        1.0144  0.5112\n",
      "     16        0.8812       0.6031        1.0074  0.5107\n",
      "     17        0.8740       0.6212        1.0100  0.5041\n",
      "     18        0.8371       0.6075        0.9889  0.5101\n",
      "     19        0.8368       0.6112        0.9693  0.5129\n",
      "     20        0.8204       0.6200        0.9785  0.5172\n",
      "     21        0.8237       0.6200        1.0397  0.5099\n",
      "     22        0.8384       0.6306        0.9225  0.5012\n",
      "     23        0.7942       0.6531        0.9291  0.5155\n",
      "     24        0.8096       0.6569        1.0454  0.5099\n",
      "     25        0.7792       0.6625        0.9540  0.5161\n",
      "     26        0.7729       0.6681        0.9160  0.5169\n",
      "     27        0.7495       0.6813        0.9091  0.5216\n",
      "     28        0.7366       0.6731        0.9763  0.5490\n",
      "     29        0.7523       0.6525        0.8861  0.5274\n",
      "     30        0.7302       0.6700        0.9526  0.5178\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4330       0.1025        2.3124  0.5213\n",
      "      2        2.3032       0.1175        2.2903  0.5490\n",
      "      3        2.2863       0.1344        2.2707  0.5127\n",
      "      4        2.2726       0.1269        2.2633  0.5329\n",
      "      5        2.2346       0.1394        2.2327  0.5443\n",
      "      6        2.1477       0.1894        2.0909  0.5261\n",
      "      7        2.0794       0.1975        2.0786  0.5068\n",
      "      8        2.0536       0.1844        2.1000  0.5089\n",
      "      9        2.0408       0.2094        2.0191  0.5401\n",
      "     10        1.9995       0.1931        2.0049  0.5225\n",
      "     11        1.9825       0.2031        1.9926  0.5362\n",
      "     12        1.9617       0.2263        1.9931  0.5196\n",
      "     13        1.9523       0.2238        1.9781  0.5497\n",
      "     14        1.9368       0.2194        1.9624  0.8041\n",
      "     15        1.9246       0.2250        1.9503  0.6269\n",
      "     16        1.9194       0.2437        1.9717  0.6872\n",
      "     17        1.9136       0.2281        1.9758  0.6110\n",
      "     18        1.9172       0.2200        1.9411  0.6218\n",
      "     19        1.8961       0.2531        1.9417  0.6168\n",
      "     20        1.8787       0.2313        1.9072  0.6177\n",
      "     21        1.8119       0.3094        1.8126  0.6153\n",
      "     22        1.7443       0.3181        1.7602  0.6525\n",
      "     23        1.7255       0.2975        1.7574  0.7445\n",
      "     24        1.6897       0.2844        1.7317  0.6526\n",
      "     25        1.6701       0.3412        1.7151  0.6116\n",
      "     26        1.6728       0.2581        1.7202  0.6342\n",
      "     27        1.6766       0.3044        1.7268  0.6009\n",
      "     28        1.6601       0.3294        1.7920  0.5926\n",
      "     29        1.6265       0.3162        1.6708  0.5199\n",
      "     30        1.6093       0.3494        1.7077  0.5098\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.0173       0.1000        2.3151  0.5240\n",
      "      2        2.2318       0.1756        2.1131  0.5118\n",
      "      3        2.0536       0.1812        2.0453  0.5421\n",
      "      4        2.0043       0.1900        1.9980  0.5150\n",
      "      5        1.9673       0.2131        1.9391  0.5150\n",
      "      6        1.8551       0.2562        1.7612  0.5146\n",
      "      7        1.6820       0.3600        1.6731  0.5067\n",
      "      8        1.5922       0.4288        1.5626  0.5148\n",
      "      9        1.5068       0.4400        1.5264  0.5155\n",
      "     10        1.4674       0.4375        1.5065  0.5286\n",
      "     11        1.4364       0.4500        1.4818  0.5211\n",
      "     12        1.4242       0.4469        1.4689  0.5934\n",
      "     13        1.3989       0.4531        1.4718  0.5934\n",
      "     14        1.3780       0.4506        1.4384  0.5590\n",
      "     15        1.3640       0.4537        1.4335  0.5293\n",
      "     16        1.3423       0.4525        1.4211  0.5499\n",
      "     17        1.3348       0.4487        1.4454  0.5455\n",
      "     18        1.3245       0.4412        1.4857  0.5350\n",
      "     19        1.3106       0.4512        1.4162  0.5340\n",
      "     20        1.3083       0.4600        1.3777  0.5377\n",
      "     21        1.2761       0.4644        1.3844  0.5452\n",
      "     22        1.2625       0.4594        1.3805  0.5399\n",
      "     23        1.2563       0.4631        1.3824  0.5287\n",
      "     24        1.2600       0.4475        1.4089  0.5139\n",
      "     25        1.2589       0.4600        1.3807  0.5115\n",
      "     26        1.2356       0.4525        1.4254  0.5632\n",
      "     27        1.2678       0.4625        1.3656  0.5475\n",
      "     28        1.2286       0.4587        1.3933  0.5163\n",
      "     29        1.2243       0.4650        1.3765  0.5324\n",
      "     30        1.2101       0.4594        1.4038  0.5855\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6704       0.1963        2.1129  0.5751\n",
      "      2        2.0881       0.1963        2.0249  0.5992\n",
      "      3        2.0274       0.1963        1.9991  0.5462\n",
      "      4        1.9783       0.2050        1.9519  0.5222\n",
      "      5        1.9355       0.2056        1.9272  0.5245\n",
      "      6        1.9133       0.2044        1.9035  0.5374\n",
      "      7        1.8769       0.2106        1.8451  0.5574\n",
      "      8        1.8581       0.2075        1.8228  0.5451\n",
      "      9        1.7631       0.3362        1.6677  0.5394\n",
      "     10        1.6163       0.3581        1.5135  0.5925\n",
      "     11        1.4323       0.3069        1.4116  0.5489\n",
      "     12        1.3723       0.4500        1.3993  0.5164\n",
      "     13        1.3449       0.3925        1.4158  0.5464\n",
      "     14        1.2902       0.4612        1.2973  0.5433\n",
      "     15        1.2254       0.5162        1.2459  0.5357\n",
      "     16        1.1649       0.5619        1.0508  0.5188\n",
      "     17        0.9351       0.7037        0.8926  0.5835\n",
      "     18        0.8205       0.7019        0.8364  0.5311\n",
      "     19        0.7424       0.6731        0.7961  0.5157\n",
      "     20        0.7671       0.7175        0.7552  0.5588\n",
      "     21        0.7464       0.7244        0.7530  0.5371\n",
      "     22        0.6492       0.7238        0.7224  0.5511\n",
      "     23        0.6207       0.7369        0.6671  0.5372\n",
      "     24        0.5938       0.7312        0.6971  0.5475\n",
      "     25        0.5920       0.7319        0.6782  0.5426\n",
      "     26        0.5698       0.7312        0.6735  0.5210\n",
      "     27        0.5673       0.7281        0.6688  0.5092\n",
      "     28        0.5367       0.7356        0.6696  0.5536\n",
      "     29        0.5134       0.7550        0.6620  0.6113\n",
      "     30        0.5014       0.7769        0.6211  0.6113\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        3.0630       0.2319        2.0190  0.6287\n",
      "      2        1.9445       0.2681        1.8863  0.5515\n",
      "      3        1.7833       0.3600        1.7401  0.5940\n",
      "      4        1.6780       0.4200        1.6235  0.5850\n",
      "      5        1.5888       0.4294        1.5757  0.5519\n",
      "      6        1.5103       0.4344        1.4903  0.5123\n",
      "      7        1.4088       0.4931        1.4208  0.5157\n",
      "      8        1.3569       0.4788        1.4077  0.5157\n",
      "      9        1.3096       0.5331        1.3161  0.5202\n",
      "     10        1.2125       0.5494        1.2183  0.5408\n",
      "     11        1.0923       0.6262        1.0979  0.5332\n",
      "     12        0.9703       0.6262        1.0152  0.5388\n",
      "     13        0.8881       0.7025        0.9421  0.5402\n",
      "     14        0.8481       0.6956        0.9124  0.6014\n",
      "     15        0.7827       0.6975        0.9147  0.5241\n",
      "     16        0.7431       0.7163        0.8502  0.5112\n",
      "     17        0.7222       0.7150        0.8318  0.5240\n",
      "     18        0.6924       0.7212        0.8152  0.5251\n",
      "     19        0.6422       0.7362        0.7748  0.5202\n",
      "     20        0.6401       0.7356        0.8057  0.5217\n",
      "     21        0.6285       0.7338        0.7752  0.5402\n",
      "     22        0.5901       0.7338        0.7760  0.5425\n",
      "     23        0.5952       0.7488        0.7509  0.5515\n",
      "     24        0.5647       0.7494        0.7739  0.5311\n",
      "     25        0.5723       0.7481        0.8010  0.5288\n",
      "     26        0.5549       0.7725        0.7855  0.5263\n",
      "     27        0.5744       0.7494        0.7386  0.5326\n",
      "     28        0.5385       0.7694        0.7392  0.5449\n",
      "     29        0.5348       0.7812        0.7368  0.5574\n",
      "     30        0.5261       0.7438        0.7065  0.5211\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.6689       0.3606        1.7690  0.5509\n",
      "      2        1.6322       0.4194        1.6298  0.5240\n",
      "      3        1.4842       0.4437        1.4370  0.5263\n",
      "      4        1.3823       0.4506        1.4832  0.5414\n",
      "      5        1.3027       0.4831        1.3138  0.5454\n",
      "      6        1.2439       0.5356        1.1951  0.5319\n",
      "      7        1.1109       0.5706        1.1940  0.5252\n",
      "      8        1.0697       0.6000        1.1174  0.5440\n",
      "      9        0.9733       0.6669        1.0170  0.5299\n",
      "     10        0.9256       0.6663        1.0073  0.5313\n",
      "     11        0.8820       0.7119        0.9484  0.5263\n",
      "     12        0.8090       0.7156        0.9718  0.5231\n",
      "     13        0.7865       0.7194        0.9003  0.5548\n",
      "     14        0.7434       0.7200        0.8979  0.5370\n",
      "     15        0.7313       0.7194        0.8532  0.5413\n",
      "     16        0.7518       0.7262        0.8885  0.5196\n",
      "     17        0.7051       0.7331        0.8647  0.5083\n",
      "     18        0.6795       0.7344        0.8277  0.5424\n",
      "     19        0.6727       0.7325        0.8685  0.5175\n",
      "     20        0.6478       0.7356        0.8298  0.5273\n",
      "     21        0.6388       0.7275        0.8810  0.5315\n",
      "     22        0.6782       0.7181        0.8247  0.5367\n",
      "     23        0.6547       0.7400        0.8087  0.5832\n",
      "     24        0.6135       0.7412        0.8176  0.5276\n",
      "     25        0.6131       0.7350        0.8009  0.5822\n",
      "     26        0.6022       0.7344        0.7869  0.5202\n",
      "     27        0.5977       0.7381        0.8113  0.5691\n",
      "     28        0.5867       0.7319        0.8177  0.5748\n",
      "     29        0.6133       0.7319        0.8689  0.5137\n",
      "     30        0.5828       0.7381        0.7810  0.5788\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4828       0.6062        1.2634  0.5744\n",
      "      2        1.0765       0.6719        0.9879  0.6844\n",
      "      3        0.9348       0.6856        0.9562  0.5509\n",
      "      4        0.8695       0.6937        0.8693  0.5849\n",
      "      5        0.8207       0.6725        0.8905  0.5207\n",
      "      6        0.7774       0.7594        0.7716  0.5625\n",
      "      7        0.7207       0.7694        0.7597  0.5462\n",
      "      8        0.6735       0.7456        0.7501  0.5458\n",
      "      9        0.5939       0.7669        0.6567  0.5426\n",
      "     10        0.5649       0.7712        0.7237  0.5266\n",
      "     11        0.5583       0.7688        0.6347  0.5305\n",
      "     12        0.5091       0.7538        0.6702  0.5307\n",
      "     13        0.4717       0.7925        0.6104  0.5483\n",
      "     14        0.4518       0.7756        0.6812  0.6482\n",
      "     15        0.4665       0.7837        0.6243  0.6408\n",
      "     16        0.4142       0.8081        0.6245  0.5810\n",
      "     17        0.4084       0.8200        0.5866  0.6260\n",
      "     18        0.4061       0.8094        0.6097  0.6344\n",
      "     19        0.3867       0.8100        0.5711  0.6122\n",
      "     20        0.3654       0.8125        0.6106  0.5498\n",
      "     21        0.3484       0.8213        0.6128  0.5315\n",
      "     22        0.3482       0.8256        0.5969  0.5766\n",
      "     23        0.3353       0.8225        0.5788  0.5486\n",
      "     24        0.3407       0.8150        0.6246  0.5546\n",
      "     25        0.3293       0.8306        0.5921  0.6114\n",
      "     26        0.3033       0.8306        0.6003  0.7265\n",
      "     27        0.3064       0.8331        0.5477  0.6936\n",
      "     28        0.2989       0.8344        0.6089  0.5530\n",
      "     29        0.3260       0.8181        0.6397  0.5521\n",
      "     30        0.3056       0.8256        0.6315  0.5199\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5826       0.3538        1.4901  0.5452\n",
      "      2        1.3900       0.5125        1.3646  0.5429\n",
      "      3        1.3012       0.5150        1.3066  0.5870\n",
      "      4        1.2638       0.5212        1.3304  0.5382\n",
      "      5        1.2447       0.5344        1.2771  0.5298\n",
      "      6        1.1849       0.5456        1.2162  0.5323\n",
      "      7        1.1558       0.5569        1.1888  0.5527\n",
      "      8        1.1230       0.5744        1.1518  0.5637\n",
      "      9        1.0255       0.6012        1.0172  0.5243\n",
      "     10        0.9252       0.6913        0.9374  0.5152\n",
      "     11        0.8602       0.6350        0.9509  0.5651\n",
      "     12        0.8494       0.7281        0.8781  0.5947\n",
      "     13        0.8184       0.7013        0.9097  0.5938\n",
      "     14        0.7826       0.7019        0.9145  0.5566\n",
      "     15        0.7773       0.7288        0.8714  0.5909\n",
      "     16        0.7365       0.7525        0.8146  0.5622\n",
      "     17        0.7270       0.7494        0.7879  0.5504\n",
      "     18        0.6653       0.7581        0.7741  0.5344\n",
      "     19        0.6470       0.7612        0.7752  0.5281\n",
      "     20        0.6189       0.7750        0.7604  0.5474\n",
      "     21        0.6190       0.7781        0.7987  0.6138\n",
      "     22        0.6142       0.7869        0.7436  0.5731\n",
      "     23        0.5772       0.7812        0.7390  0.5631\n",
      "     24        0.5506       0.7919        0.7390  0.5790\n",
      "     25        0.5492       0.7969        0.7106  0.6117\n",
      "     26        0.4975       0.8063        0.7592  0.6036\n",
      "     27        0.4741       0.8013        0.6774  0.5760\n",
      "     28        0.4255       0.7925        0.6764  0.5843\n",
      "     29        0.4390       0.7688        0.7428  0.5662\n",
      "     30        0.4005       0.8119        0.5902  0.5822\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.4296       0.5669        1.3519  0.5885\n",
      "      2        1.1589       0.7063        1.0847  0.6082\n",
      "      3        0.9455       0.6925        1.0308  0.6269\n",
      "      4        0.8134       0.7050        0.9332  0.5657\n",
      "      5        0.7573       0.7325        0.8624  0.5393\n",
      "      6        0.6561       0.7731        0.7485  0.5405\n",
      "      7        0.6415       0.7750        0.8312  0.5404\n",
      "      8        0.5796       0.7706        0.7094  0.5970\n",
      "      9        0.5443       0.7625        0.7128  0.5763\n",
      "     10        0.5105       0.7650        0.7896  0.6040\n",
      "     11        0.5190       0.7794        0.6752  0.5545\n",
      "     12        0.4722       0.7819        0.6931  0.6260\n",
      "     13        0.4265       0.7987        0.6307  0.5920\n",
      "     14        0.4186       0.7956        0.6346  0.5722\n",
      "     15        0.3830       0.8181        0.6558  0.5518\n",
      "     16        0.3743       0.8125        0.5870  0.6083\n",
      "     17        0.3249       0.8337        0.5707  0.7261\n",
      "     18        0.3182       0.8137        0.5972  0.6341\n",
      "     19        0.3407       0.8344        0.5799  0.6529\n",
      "     20        0.3222       0.8313        0.5958  0.5920\n",
      "     21        0.3101       0.8331        0.5889  0.5503\n",
      "     22        0.2816       0.8094        0.7182  0.5135\n",
      "     23        0.3134       0.8356        0.5519  0.5537\n",
      "     24        0.2719       0.8325        0.6234  0.6117\n",
      "     25        0.2689       0.8387        0.5722  0.6015\n",
      "     26        0.2517       0.8319        0.5899  0.6236\n",
      "     27        0.2694       0.8237        0.6192  0.5602\n",
      "     28        0.2399       0.8488        0.5536  0.5675\n",
      "     29        0.2315       0.8425        0.5991  0.5427\n",
      "     30        0.2391       0.8150        0.6721  0.5389\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.5010       0.6188        1.2370  0.5521\n",
      "      2        0.9974       0.7300        0.8643  0.5623\n",
      "      3        0.7764       0.7281        0.8060  0.5767\n",
      "      4        0.6881       0.7744        0.7179  0.5321\n",
      "      5        0.6464       0.7681        0.7003  0.5503\n",
      "      6        0.5795       0.7913        0.7039  0.5240\n",
      "      7        0.5244       0.7837        0.6617  0.5452\n",
      "      8        0.4959       0.7856        0.7316  0.5150\n",
      "      9        0.4865       0.8075        0.6407  0.5317\n",
      "     10        0.4350       0.7900        0.7168  0.5413\n",
      "     11        0.4306       0.7975        0.6437  0.5267\n",
      "     12        0.4309       0.8131        0.6536  0.5248\n",
      "     13        0.4095       0.8131        0.5933  0.5290\n",
      "     14        0.4134       0.8113        0.6178  0.5323\n",
      "     15        0.3708       0.8031        0.6020  0.5914\n",
      "     16        0.3759       0.8013        0.6451  0.5340\n",
      "     17        0.3544       0.8387        0.6011  0.5109\n",
      "     18        0.3701       0.7869        0.7170  0.5310\n",
      "     19        0.3718       0.8250        0.6181  0.5790\n",
      "     20        0.3267       0.8100        0.6413  0.5872\n",
      "     21        0.3501       0.8094        0.6491  0.5262\n",
      "     22        0.3161       0.8206        0.6218  0.5705\n",
      "     23        0.3027       0.8250        0.5928  0.7278\n",
      "     24        0.2989       0.8256        0.6807  0.6753\n",
      "     25        0.3066       0.8137        0.7321  0.5508\n",
      "     26        0.3263       0.8244        0.6115  0.5616\n",
      "     27        0.2920       0.8087        0.6718  0.5541\n",
      "     28        0.2795       0.8200        0.6093  0.6458\n",
      "     29        0.2792       0.8269        0.6382  0.5702\n",
      "     30        0.2843       0.8169        0.6334  0.5455\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.1311       0.5500        1.2783  0.6779\n",
      "      2        1.0870       0.5981        1.0421  0.5671\n",
      "      3        0.8970       0.6819        0.8297  0.5471\n",
      "      4        0.6994       0.7275        0.8015  0.5739\n",
      "      5        0.6227       0.7631        0.6554  0.5374\n",
      "      6        0.5599       0.7856        0.6460  0.5600\n",
      "      7        0.5322       0.7937        0.6158  0.5480\n",
      "      8        0.5078       0.8031        0.5690  0.5428\n",
      "      9        0.4371       0.8100        0.5470  0.5672\n",
      "     10        0.4266       0.8263        0.5319  0.5375\n",
      "     11        0.4088       0.8306        0.5273  0.5472\n",
      "     12        0.4021       0.8250        0.5333  0.5450\n",
      "     13        0.3784       0.8375        0.5189  0.5375\n",
      "     14        0.3578       0.8294        0.5102  0.5550\n",
      "     15        0.3416       0.8213        0.5886  0.5572\n",
      "     16        0.3853       0.8156        0.5534  0.5435\n",
      "     17        0.3582       0.8175        0.5736  0.5442\n",
      "     18        0.3463       0.8294        0.5183  0.5165\n",
      "     19        0.3131       0.8287        0.5852  0.5428\n",
      "     20        0.3105       0.8381        0.5278  0.5471\n",
      "     21        0.3165       0.8044        0.6341  0.5428\n",
      "     22        0.2977       0.8287        0.5813  0.5163\n",
      "     23        0.2943       0.8413        0.5322  0.5228\n",
      "     24        0.2997       0.8413        0.5354  0.5972\n",
      "     25        0.2734       0.8244        0.6009  0.5456\n",
      "     26        0.2907       0.8369        0.5807  0.5548\n",
      "     27        0.2951       0.8425        0.5486  0.8748\n",
      "     28        0.2436       0.8331        0.5392  1.5626\n",
      "     29        0.2507       0.8331        0.5611  0.7796\n",
      "     30        0.2227       0.8438        0.5612  0.8413\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "Re-initializing module because the following parameters were re-set: l1, l2.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        2.9519       0.4419        1.5696  0.6223\n",
      "      2        1.3364       0.6494        1.1013  0.8784\n",
      "      3        0.9893       0.7037        0.8216  0.7682\n",
      "      4        0.7516       0.7456        0.7342  0.6922\n",
      "      5        0.6527       0.7412        0.6943  0.6628\n",
      "      6        0.5965       0.7625        0.6400  0.7754\n",
      "      7        0.5526       0.7606        0.6771  0.7305\n",
      "      8        0.5037       0.8169        0.5702  0.6535\n",
      "      9        0.4773       0.8131        0.5965  0.6366\n",
      "     10        0.4539       0.8144        0.6095  0.5546\n",
      "     11        0.4448       0.8187        0.5770  0.5778\n",
      "     12        0.4192       0.8131        0.5915  0.7001\n",
      "     13        0.3744       0.8150        0.5220  0.5645\n",
      "     14        0.3677       0.8200        0.5495  0.5508\n",
      "     15        0.3978       0.7994        0.6137  0.5814\n",
      "     16        0.3657       0.8400        0.5578  0.5668\n",
      "     17        0.3486       0.7894        0.6117  0.6585\n",
      "     18        0.3246       0.8400        0.5307  0.5734\n",
      "     19        0.3261       0.8300        0.5613  0.5331\n",
      "     20        0.3098       0.8200        0.5459  0.5248\n",
      "     21        0.3301       0.8156        0.5982  0.5486\n",
      "     22        0.3013       0.8313        0.5376  0.8110\n",
      "     23        0.2978       0.8375        0.5676  0.5175\n",
      "     24        0.2813       0.8131        0.6344  0.5153\n",
      "     25        0.2938       0.8287        0.5685  0.5259\n",
      "     26        0.2732       0.8250        0.6213  0.5107\n",
      "     27        0.2883       0.8200        0.6218  0.5148\n",
      "     28        0.2747       0.8294        0.5517  0.5323\n",
      "     29        0.2416       0.8325        0.6084  0.6948\n",
      "     30        0.2985       0.8350        0.5696  0.6116\n",
      "0.826 {'module__l1': 75, 'module__l2': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier\n",
    "net = NeuralNetClassifier(\n",
    "        MyModule(l1=50, l2=10),\n",
    "        max_epochs=30,\n",
    "        lr=0.001,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        iterator_train__shuffle=True,\n",
    "        )\n",
    "params = {\n",
    "    'module__l1': [50,75,100],#,500],\n",
    "    'module__l2': [10,15,20],#100]\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=5, scoring='accuracy')\n",
    "gs.fit(X_train,y_train)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "        MyModule(l1=50, l2=20),\n",
    "        max_epochs=30,\n",
    "        lr=0.001,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        iterator_train__shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.9651\u001b[0m       \u001b[32m0.5065\u001b[0m        \u001b[35m1.4072\u001b[0m  0.3910\n",
      "      2        \u001b[36m1.2010\u001b[0m       \u001b[32m0.6825\u001b[0m        \u001b[35m1.1376\u001b[0m  0.3590\n",
      "      3        \u001b[36m0.9166\u001b[0m       \u001b[32m0.7160\u001b[0m        \u001b[35m0.8362\u001b[0m  0.3570\n",
      "      4        \u001b[36m0.6706\u001b[0m       \u001b[32m0.7655\u001b[0m        \u001b[35m0.6665\u001b[0m  0.3550\n",
      "      5        \u001b[36m0.6089\u001b[0m       \u001b[32m0.7790\u001b[0m        \u001b[35m0.6176\u001b[0m  0.3590\n",
      "      6        \u001b[36m0.5452\u001b[0m       0.7745        0.6498  0.3535\n",
      "      7        \u001b[36m0.5328\u001b[0m       \u001b[32m0.7860\u001b[0m        \u001b[35m0.5889\u001b[0m  0.3715\n",
      "      8        \u001b[36m0.5297\u001b[0m       \u001b[32m0.7980\u001b[0m        \u001b[35m0.5812\u001b[0m  0.3450\n",
      "      9        \u001b[36m0.4799\u001b[0m       0.7965        0.5929  0.3630\n",
      "     10        \u001b[36m0.4545\u001b[0m       \u001b[32m0.8050\u001b[0m        \u001b[35m0.5710\u001b[0m  0.3480\n",
      "     11        \u001b[36m0.4311\u001b[0m       0.7985        0.5869  0.3460\n",
      "     12        0.4483       0.8035        \u001b[35m0.5665\u001b[0m  0.3615\n",
      "     13        0.4445       0.7785        0.6365  0.3590\n",
      "     14        \u001b[36m0.4144\u001b[0m       0.8010        0.5767  0.3520\n",
      "     15        \u001b[36m0.3821\u001b[0m       0.8000        0.5721  0.3570\n",
      "     16        0.3888       \u001b[32m0.8195\u001b[0m        0.5731  0.3510\n",
      "     17        \u001b[36m0.3675\u001b[0m       \u001b[32m0.8310\u001b[0m        \u001b[35m0.5278\u001b[0m  0.3610\n",
      "     18        \u001b[36m0.3426\u001b[0m       0.8225        0.5601  0.3660\n",
      "     19        \u001b[36m0.3415\u001b[0m       0.8290        0.5855  0.3500\n",
      "     20        0.3496       \u001b[32m0.8360\u001b[0m        \u001b[35m0.5219\u001b[0m  0.3690\n",
      "     21        \u001b[36m0.3145\u001b[0m       \u001b[32m0.8385\u001b[0m        \u001b[35m0.5165\u001b[0m  0.3460\n",
      "     22        0.3162       \u001b[32m0.8395\u001b[0m        0.5550  0.3510\n",
      "     23        \u001b[36m0.2981\u001b[0m       0.8165        0.5832  0.3580\n",
      "     24        0.3039       0.8185        0.6375  0.3620\n",
      "     25        0.3219       0.8175        0.6753  0.3650\n",
      "     26        0.3272       \u001b[32m0.8530\u001b[0m        \u001b[35m0.5021\u001b[0m  0.3540\n",
      "     27        \u001b[36m0.2837\u001b[0m       0.8490        0.5205  0.3460\n",
      "     28        0.2873       0.8325        0.5692  0.3530\n",
      "     29        0.2908       0.8485        0.5414  0.3630\n",
      "     30        \u001b[36m0.2757\u001b[0m       0.8470        0.5211  0.3540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (input): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (hidden1): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (hidden2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X-axis is predicted label for test data and Y-axis is true label for test data\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "a = y_pred_test # predicted\n",
    "b = y_test.numpy()   #real label\n",
    "print(\"\\nX-axis is predicted label for test data and Y-axis is true label for test data\\n\")\n",
    "classes = np.array([str(i) for i in range(0,10)])\n",
    "c = confusion_matrix(a,b)\n",
    "sns.heatmap(c, annot=True, xticklabels=classes, yticklabels=classes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.833\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       107\n",
      "           1       0.98      0.94      0.96       105\n",
      "           2       0.68      0.87      0.76       111\n",
      "           3       0.77      0.88      0.82        93\n",
      "           4       0.76      0.64      0.70       115\n",
      "           5       0.95      0.84      0.89        87\n",
      "           6       0.69      0.55      0.61        97\n",
      "           7       0.87      0.98      0.92        95\n",
      "           8       0.95      0.93      0.94        95\n",
      "           9       0.96      0.95      0.95        95\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.84      0.84      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true=y_test,y_pred=y_pred_test)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
